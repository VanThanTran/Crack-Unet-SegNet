{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary gdown\n#!pip install albumentations==0.4.6","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:20:05.191240Z","iopub.execute_input":"2022-01-29T10:20:05.191560Z","iopub.status.idle":"2022-01-29T10:20:14.945138Z","shell.execute_reply.started":"2022-01-29T10:20:05.191523Z","shell.execute_reply":"2022-01-29T10:20:14.944008Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#import warnings\n#warnings.simplefilter('error', UserWarning)\n\n#from IPython.core.interactiveshell import InteractiveShell\n#InteractiveShell.ast_node_interactivity = \"all\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom skimage import io\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as tfs\nimport torchvision.models\nfrom torchsummary import summary\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nfrom IPython.display import clear_output\nfrom tqdm.notebook import tqdm\nimport pickle\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:20:22.635130Z","iopub.execute_input":"2022-01-29T10:20:22.635511Z","iopub.status.idle":"2022-01-29T10:20:27.353625Z","shell.execute_reply.started":"2022-01-29T10:20:22.635464Z","shell.execute_reply":"2022-01-29T10:20:27.352678Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ncontent_dir = '/kaggle/working/PH2Dataset'\nbatch_size = 16","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:20:32.361660Z","iopub.execute_input":"2022-01-29T10:20:32.362406Z","iopub.status.idle":"2022-01-29T10:20:32.419456Z","shell.execute_reply.started":"2022-01-29T10:20:32.362366Z","shell.execute_reply":"2022-01-29T10:20:32.418422Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data","metadata":{}},{"cell_type":"code","source":"if not os.path.exists(content_dir):\n    print('Download dataset...')\n    # !wget --quiet https://www.dropbox.com/s/k88qukc20ljnbuo/PH2Dataset.rar\n    !gdown https://drive.google.com/uc?id=1JHRw-LcPoHoApGNoub2b-DgrSWnRw-ZB -O PH2Dataset.zip\n    # print('Install unrar...', end='')\n    # !apt-get -qq install unrar > /dev/null 2>&1\n    # print('done.')\n    print('Extract archive files...', end='')\n    #!unrar x -idq PH2Dataset.rar\n    !unzip -q PH2Dataset.zip\n    print('done.')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:20:52.773527Z","iopub.execute_input":"2022-01-29T10:20:52.773822Z","iopub.status.idle":"2022-01-29T10:20:58.205693Z","shell.execute_reply.started":"2022-01-29T10:20:52.773781Z","shell.execute_reply":"2022-01-29T10:20:58.204418Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class CustomDataSet(Dataset):\n    def __init__(self, main_dir, transform):\n        self.transform = transform\n        self.samples = []\n        self.labels = []\n        for root, dirs, files in os.walk(main_dir):\n            if root.endswith('_Dermoscopic_Image'):\n                self.samples.append(os.path.join(root, files[0]))\n            if root.endswith('_lesion'):\n                self.labels.append(os.path.join(root, files[0]))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        # image = Image.open(self.samples[idx])\n        # label = Image.open(self.labels[idx])\n        image = cv2.imread(self.samples[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = cv2.imread(self.labels[idx], cv2.IMREAD_GRAYSCALE)\n        transformed = self.transform(image=image, mask=label)\n        tensor_image = transformed['image'].transpose(2, 0, 1)\n        label_image = transformed['mask'][np.newaxis, :]\n        tensor_image = torch.FloatTensor(tensor_image) / 255\n        label_image = torch.FloatTensor(label_image) / 255\n        #print(tensor_image.shape, label_image.shape)\n        #print(tensor_image.min(), tensor_image.max(), label_image.min(), label_image.max())\n        return tensor_image, label_image\n    \n#     def __getitem__(self, idx):\n#         image = Image.open(self.samples[idx])\n#         tensor_image = self.transform(image)\n#         label = Image.open(self.labels[idx])\n#         label_image = self.transform(label)\n#         print(tensor_image.min(), tensor_image.max(), label_image.min(), label_image.max())\n#         return tensor_image, label_image","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:21:05.227561Z","iopub.execute_input":"2022-01-29T10:21:05.227963Z","iopub.status.idle":"2022-01-29T10:21:05.240775Z","shell.execute_reply.started":"2022-01-29T10:21:05.227889Z","shell.execute_reply":"2022-01-29T10:21:05.239592Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"transform = A.Compose([\n    A.Resize(256, 256),\n#     A.HorizontalFlip(),\n#     A.VerticalFlip(),\n#     A.augmentations.geometric.Affine(\n#         scale={'x':(1.0, 1.2), 'y':(1.0, 1.2)},\n# #        cval=[55, 51, 49],\n# #        mode=cv2.BORDER_CONSTANT\n#     )\n    #A.ToFloat(),\n    #A.pytorch.ToTensorV2(),\n    #A.Normalize(mean=0, std=1),\n    #tfs.RandomAffine(10,translate=(0.1, 0.1), scale=(0.9, 1.1)),\n])\n\n# transform = tfs.Compose([\n#     tfs.Resize((256, 256)),\n#     tfs.ToTensor(),\n# ])\n\ndermoscopic_dataset = CustomDataSet(os.path.join(content_dir, 'PH2 Dataset images'), transform=transform)\n\nidx = np.random.choice(len(dermoscopic_dataset), len(dermoscopic_dataset), False)\n#train_idx, valid_idx, test_idx = np.split(idx, [50, 70])\ntrain_idx, test_idx = np.split(idx, [70])\n\ndataset_train = Subset(dermoscopic_dataset, train_idx)\n#dataset_valid = Subset(dermoscopic_dataset, valid_idx)\ndataset_test  = Subset(dermoscopic_dataset, test_idx)\n\ndataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n#dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=True)\ndataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:21:11.594267Z","iopub.execute_input":"2022-01-29T10:21:11.594927Z","iopub.status.idle":"2022-01-29T10:21:11.638209Z","shell.execute_reply.started":"2022-01-29T10:21:11.594858Z","shell.execute_reply":"2022-01-29T10:21:11.637250Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"def show_dermoscopic_imgs(images, labels, threshold=None):\n    images = images.numpy().transpose(0, 2, 3, 1)\n    labels = labels.numpy().transpose(0, 2, 3, 1)\n    if threshold is not None:\n        labels = np.where(labels > threshold, 1, 0)\n    plt.figure(figsize=(18, 6))\n    for i in range(4):\n        plt.subplot(2, 6, i+1)\n        plt.imshow(images[i])\n        plt.axis(\"off\")\n\n        plt.subplot(2, 6, i+7)\n        plt.imshow(labels[i], cmap='gray')\n        plt.axis(\"off\")\n    # plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:21:21.154312Z","iopub.execute_input":"2022-01-29T10:21:21.154631Z","iopub.status.idle":"2022-01-29T10:21:21.165901Z","shell.execute_reply.started":"2022-01-29T10:21:21.154600Z","shell.execute_reply":"2022-01-29T10:21:21.162406Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"images, labels = next(iter(dataloader_train))\nshow_dermoscopic_imgs(images, labels)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:21:24.492372Z","iopub.execute_input":"2022-01-29T10:21:24.492665Z","iopub.status.idle":"2022-01-29T10:21:25.700992Z","shell.execute_reply.started":"2022-01-29T10:21:24.492634Z","shell.execute_reply":"2022-01-29T10:21:25.699929Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def show_loss(history):\n    plt.figure(figsize=(12, 8))\n    plt.plot(history)\n    plt.title('Loss')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:21:29.456776Z","iopub.execute_input":"2022-01-29T10:21:29.457495Z","iopub.status.idle":"2022-01-29T10:21:29.463006Z","shell.execute_reply.started":"2022-01-29T10:21:29.457454Z","shell.execute_reply":"2022-01-29T10:21:29.461707Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Metrics\n\nGood summary: [A survey of loss functions for semantic segmentation](https://arxiv.org/pdf/2006.14822.pdf)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:13:39.409414Z","iopub.execute_input":"2021-11-18T13:13:39.409711Z","iopub.status.idle":"2021-11-18T13:13:39.413556Z","shell.execute_reply.started":"2021-11-18T13:13:39.409677Z","shell.execute_reply":"2021-11-18T13:13:39.412683Z"}}},{"cell_type":"markdown","source":"## IoU scoring metric\n\n$$I o U=\\frac{\\text {target } \\cap \\text { prediction }}{\\text {target } \\cup{prediction }}$$","metadata":{}},{"cell_type":"code","source":"SMOOTH = 1e-6\n# https://www.kaggle.com/iezepov/fast-iou-scoring-metric-in-pytorch-and-numpy\ndef iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n    outputs = outputs.byte()\n    labels = labels.byte()\n    #print(outputs)\n    # You can comment out this line if you are passing tensors of equal shape\n    # But if you are passing output from UNet or something it will most probably\n    # be with the BATCH x 1 x H x W shape\n    #outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n    # thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n    # return thresholded  # Or thresholded.mean() if you are interested in average across the batch\n    return iou","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:21:34.576060Z","iopub.execute_input":"2022-01-29T10:21:34.577010Z","iopub.status.idle":"2022-01-29T10:21:34.586856Z","shell.execute_reply.started":"2022-01-29T10:21:34.576964Z","shell.execute_reply":"2022-01-29T10:21:34.585617Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## BCE Loss\n\n$$\\mathcal L_{BCE}(y, \\hat y) = -\\sum_i \\left[y_i\\log\\sigma(\\hat y_i) + (1-y_i)\\log(1-\\sigma(\\hat y_i))\\right]$$","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:27:55.204072Z","iopub.execute_input":"2021-11-18T13:27:55.204598Z","iopub.status.idle":"2021-11-18T13:27:55.207505Z","shell.execute_reply.started":"2021-11-18T13:27:55.204565Z","shell.execute_reply":"2021-11-18T13:27:55.206925Z"}}},{"cell_type":"code","source":"class BCELoss_classic(nn.Module):\n    def __init__(self, reduction='mean'):\n        super().__init__()\n        if reduction not in ('mean', 'sum'):\n            raise ValueError('\"{}\" is not a valid mode for reduction. Only \"mean\"'\n                             'and \"sum\" are allowed.'.format(rediction))\n        self.reduction = reduction\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        # outputs = torch.clamp(outputs, 0, 1)\n        outputs = torch.sigmoid(outputs)\n        bce = labels * torch.log(outputs + SMOOTH) + (1 - labels) * torch.log(1 - outputs + SMOOTH)\n        if self.reduction == 'mean':\n            return -torch.mean(bce)\n        elif self.reduction == 'sum':\n            return -torch.sum(bce)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:21:57.657863Z","iopub.execute_input":"2022-01-29T10:21:57.658180Z","iopub.status.idle":"2022-01-29T10:21:57.667387Z","shell.execute_reply.started":"2022-01-29T10:21:57.658143Z","shell.execute_reply":"2022-01-29T10:21:57.666014Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"$$\\mathcal L_{BCE}(y, \\hat y) = \\hat y - y\\hat y + \\log\\left(1+\\exp(-\\hat y)\\right)$$","metadata":{}},{"cell_type":"code","source":"class BCELoss_with_logits(nn.Module):\n    def __init__(self, reduction='mean', truncate=False):\n        super().__init__()\n        if reduction not in ('mean', 'sum'):\n            raise ValueError('\"{}\" is not a valid mode for reduction. Only \"mean\"'\n                             'and \"sum\" are allowed.'.format(rediction))\n        self.reduction = reduction\n        self.truncate = truncate\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        # classical without sigmoid\n        # but you need to cut off large negative logits, otherwise it will be -inf -> nan \n        if self.truncate:\n            outputs = torch.sigmoid(outputs)\n        outputs = outputs.float()\n        labels = labels.float()\n        # print(torch.min(outputs))\n        bce = outputs - labels * outputs + torch.log(1 + torch.exp(-outputs))\n        if self.reduction == 'mean':\n            return torch.mean(bce)\n        elif self.reduction == 'sum':\n            return torch.sum(bce)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:22:00.651695Z","iopub.execute_input":"2022-01-29T10:22:00.652841Z","iopub.status.idle":"2022-01-29T10:22:00.662506Z","shell.execute_reply.started":"2022-01-29T10:22:00.652791Z","shell.execute_reply":"2022-01-29T10:22:00.661284Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# torch.mean(torch.log(1 + torch.exp(torch.tensor(255))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = torch.Tensor([\n    [-0.4717,  0.8484,  0.7424],\n    [ 0.0880,  0.1379,  0.8387],\n    [ 0.3874, -1.8205,  1.5422]\n])\ntarget = torch.Tensor([\n    [0, 1, 0],\n    [1, 0, 1],\n    [0, 1, 0]\n])\nsigm = nn.Sigmoid()\n\nprint(\n    output,\n    target,\n    nn.BCELoss(reduction='mean')(sigm(output), target),\n    BCELoss_classic(reduction='mean')(output, target),\n    BCELoss_with_logits(reduction='mean')(output, target),\n    sep='\\n'\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:22:03.956055Z","iopub.execute_input":"2022-01-29T10:22:03.957266Z","iopub.status.idle":"2022-01-29T10:22:04.051554Z","shell.execute_reply.started":"2022-01-29T10:22:03.957178Z","shell.execute_reply":"2022-01-29T10:22:04.050170Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Dice Loss\n\n$$D(X,Y)=\\frac{2|X\\cap Y|}{|X|+|Y|}$$\n\n$$\\mathcal L_D(X,Y) = 1-\\frac{1}{256 \\times 256} \\times \\sum_i\\frac{2X_iY_i}{X_i+Y_i}.$$","metadata":{}},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, reduction='mean'):\n        super().__init__()\n\n#     # not work, why?\n#     def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n#         coef = 1 / (outputs.shape[0] * outputs.shape[2] * outputs.shape[3])\n#         outputs = torch.sigmoid(outputs)\n#         num = 2 * outputs * labels\n#         den = outputs + labels\n#         res = 1 - coef * ((num + 1)/(den + 1)).sum()\n#         return res\n\n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        outputs = torch.sigmoid(outputs)\n        num = 2 * (outputs * labels).sum()\n        den = (outputs + labels).sum()\n        res = 1 - (num + SMOOTH) / (den + SMOOTH)\n        return res","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:22:07.808324Z","iopub.execute_input":"2022-01-29T10:22:07.809352Z","iopub.status.idle":"2022-01-29T10:22:07.818023Z","shell.execute_reply.started":"2022-01-29T10:22:07.809303Z","shell.execute_reply":"2022-01-29T10:22:07.816916Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Focal Loss\n\n$$FL(p_t) = -\\alpha_t(1-p_t)^\\gamma log(p_t)$$\n$$CE(p,y) = CE(p_t) = -log(p_t)$$\n$$p_t = e^{-CE(p_t)}$$\n$$FL(p_t) = \\alpha_t(1-e^{-CE(p_t)})^\\gamma CE(p_t)$$","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:15:44.629785Z","iopub.execute_input":"2021-11-21T09:15:44.630127Z","iopub.status.idle":"2021-11-21T09:15:44.635131Z","shell.execute_reply.started":"2021-11-21T09:15:44.630085Z","shell.execute_reply":"2021-11-21T09:15:44.634061Z"}}},{"cell_type":"code","source":"# https://arxiv.org/pdf/1708.02002.pdf\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha: int = 1, gamma: int = 2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        bce_logit = nn.BCEWithLogitsLoss()\n        ce = bce_logit(outputs, labels)\n        pt = torch.exp(-ce)\n        fl = self.alpha * torch.pow((1 - pt), self.gamma) * ce\n        return fl","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:22:11.089027Z","iopub.execute_input":"2022-01-29T10:22:11.089470Z","iopub.status.idle":"2022-01-29T10:22:11.099770Z","shell.execute_reply.started":"2022-01-29T10:22:11.089435Z","shell.execute_reply":"2022-01-29T10:22:11.098668Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Tversky Loss\n\n[Tversky loss function for image segmentation using 3D fully convolutional deep networks](https://arxiv.org/abs/1706.05721)\n\n$$TI(p,\\hat p) = \\frac{p\\hat p}{p\\hat p + \\beta(1 − p)\\hat p + (1 − \\beta)p(1 − \\hat p)}$$\n$$TL(p,\\hat p) = 1 - \\frac{1 + p\\hat p}{1+ p\\hat p + \\beta(1 − p)\\hat p + (1 − \\beta)p(1 − \\hat p)}$$","metadata":{}},{"cell_type":"code","source":"class TverskyLoss(nn.Module):\n    def __init__(self, alpha:int = 0.5, beta:int = 0.5):\n        super().__init__()\n        self.alpha = alpha\n        self.beta = beta\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        outputs = torch.sigmoid(outputs)\n        pp = (labels * outputs).sum()\n        den1 = self.alpha * ((1 - labels) * outputs).sum()\n        den2 = self.beta * (labels * (1 - outputs)).sum()\n        tl = 1 - (1 + pp) / (1 + pp + den1 + den2)\n        return tl       ","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:22:14.600010Z","iopub.execute_input":"2022-01-29T10:22:14.600643Z","iopub.status.idle":"2022-01-29T10:22:14.618431Z","shell.execute_reply.started":"2022-01-29T10:22:14.600604Z","shell.execute_reply":"2022-01-29T10:22:14.610293Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Focal Tversky Loss","metadata":{}},{"cell_type":"code","source":"class FocalTverskyLoss(nn.Module):\n    def __init__(self, alpha:int = 0.5, beta:int = 0.5, gamma:int = 2):\n        super().__init__()\n        self.gamma = gamma\n        self.tl = TverskyLoss(alpha, beta)\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        tl = self.tl(outputs, labels)\n        return torch.pow(tl, self.gamma)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:22:18.455853Z","iopub.execute_input":"2022-01-29T10:22:18.456271Z","iopub.status.idle":"2022-01-29T10:22:18.464485Z","shell.execute_reply.started":"2022-01-29T10:22:18.456152Z","shell.execute_reply":"2022-01-29T10:22:18.462777Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Lovasz Loss","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/67791\nclass LovaszLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        #outputs = torch.sigmoid(outputs)\n        outputs = outputs.flatten()\n        labels = labels.flatten()\n        signs = 2 * labels.float() - 1\n        errors = (1 - outputs * signs)\n        errors_sorted, indices = torch.sort(errors, dim=0, descending=True)\n        gt_sorted = labels[indices.data]\n\n        # gradient\n        gts = gt_sorted.sum()\n        intersection = gts - gt_sorted.float().cumsum(0)\n        union = gts + (1 - gt_sorted).float().cumsum(0)\n        grad = 1. - intersection / union\n\n        p = len(gt_sorted)\n        grad[1:p] = grad[1:p] - grad[0:-1]\n       \n        loss = torch.dot(torch.relu(errors_sorted), grad)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:22:21.905456Z","iopub.execute_input":"2022-01-29T10:22:21.905744Z","iopub.status.idle":"2022-01-29T10:22:21.915437Z","shell.execute_reply.started":"2022-01-29T10:22:21.905714Z","shell.execute_reply":"2022-01-29T10:22:21.914430Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Train/valid functions","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:33:36.596973Z","iopub.execute_input":"2021-11-21T13:33:36.598099Z","iopub.status.idle":"2021-11-21T13:33:36.625852Z","shell.execute_reply.started":"2021-11-21T13:33:36.597927Z","shell.execute_reply":"2021-11-21T13:33:36.624662Z"}}},{"cell_type":"code","source":"def score_model(model, metric, data, threshold=0):\n    model.to(device).eval() # testing mode\n    scores = 0\n    threshold = torch.tensor(threshold).to(device)\n    for X_batch, Y_label in data:\n        X_batch = X_batch.to(device)\n        with torch.no_grad():\n            Y_pred = model(X_batch)\n        scores += metric((Y_pred > threshold), Y_label.to(device)).mean().item()\n    return scores/len(data)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:22:28.505010Z","iopub.execute_input":"2022-01-29T10:22:28.505686Z","iopub.status.idle":"2022-01-29T10:22:28.513251Z","shell.execute_reply.started":"2022-01-29T10:22:28.505652Z","shell.execute_reply":"2022-01-29T10:22:28.511949Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def train_model(\n    model: torch.nn.Module,\n    criterion: torch.nn.Module,\n    optimizer: torch.nn.Module,\n    dataloader_train: torch.utils.data.DataLoader,\n    dataloader_test: torch.utils.data.DataLoader,\n    epochs: int = 100\n) -> (torch.nn.Module, dict):\n    r\"\"\"Training the model. Returns list of train losses.\n    Args:\n        model (torch.nn.Module): Neural network\n        criterion (torch.nn.Module): Cost function\n        optimizer (torch.nn.Module): Optimization algorithm\n        dataloader_train: (torch.utils.data.DataLoader): Train data\n        dataloader_valid: (torch.utils.data.DataLoader): Valid data\n        epochs (int): Number of training iterations. Default: 20\n    \"\"\"\n#def train(model, optimizer, loss_fn, epochs, data_tr, data_val):\n    X_val, Y_val = next(iter(dataloader_test))\n    losses = []\n    metric = []\n  \n    for epoch in range(epochs):\n        avg_loss = 0\n        model.train()  # train mode\n        for X_batch, Y_batch in tqdm(dataloader_train, desc='Progress'):\n            # data to device\n            X_batch = X_batch.to(device)\n            Y_batch = Y_batch.to(device)\n            # set parameter gradients to zero\n            optimizer.zero_grad()\n            # forward\n            # print(Y_batch.shape)\n            Y_pred = model(X_batch)\n            loss = criterion(Y_pred, Y_batch)\n            print(loss)\n            loss.backward() # backward-pass\n            optimizer.step()  # update weights\n            # calculate loss to show the user\n            avg_loss += loss / len(dataloader_train)\n\n        losses.append(avg_loss.item())\n        metric.append(score_model(model, iou_pytorch, dataloader_test))\n        \n        # show intermediate results\n        model.eval()  # testing mode\n        # detach and put into cpu\n        Y_hat = model(X_val.to(device)).detach().cpu()\n        # Visualize tools\n        clear_output(wait=True)\n        show_dermoscopic_imgs(X_val, Y_hat, threshold=0.1)\n        plt.title('%d / %d - loss: %f' % (epoch+1, epochs, avg_loss))\n        plt.show()\n    return losses, metric","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:22:37.915730Z","iopub.execute_input":"2022-01-29T10:22:37.916513Z","iopub.status.idle":"2022-01-29T10:22:37.929261Z","shell.execute_reply.started":"2022-01-29T10:22:37.916473Z","shell.execute_reply":"2022-01-29T10:22:37.927775Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def predict(model, data):\n    model.to(device).eval()  # testing mode\n    with torch.no_grad():\n        Y_pred = [model(X_batch.to(device)) for X_batch, _ in data]\n    return Y_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:22:45.057914Z","iopub.execute_input":"2022-01-29T10:22:45.058238Z","iopub.status.idle":"2022-01-29T10:22:45.066509Z","shell.execute_reply.started":"2022-01-29T10:22:45.058206Z","shell.execute_reply":"2022-01-29T10:22:45.065334Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# SegNet","metadata":{}},{"cell_type":"code","source":"# vgg16 = torchvision.models.vgg16_bn()\n# vgg16.features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SegNet(nn.Module):\n    def _enc_layer(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n    \n    def _dec_layer(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n  \n    def __init__(self):\n        super().__init__()\n\n        # encoder (downsampling)\n        # Each enc_conv/dec_conv block should look like this:\n        # nn.Sequential(\n        #     nn.Conv2d(...),\n        #     ... (2 or 3 conv layers with relu and batchnorm),\n        # )\n\n        self.enc_conv0 = nn.Sequential(self._enc_layer(3, 64), self._enc_layer(64, 64))\n        self.pool0 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True) # 256 -> 128\n        self.enc_conv1 = nn.Sequential(self._enc_layer(64, 128), self._enc_layer(128, 128))\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True) # 128 -> 64\n        self.enc_conv2 = nn.Sequential(self._enc_layer(128, 256), self._enc_layer(256, 256), self._enc_layer(256, 256))\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True) # 64 -> 32\n        self.enc_conv3 = nn.Sequential(self._enc_layer(256, 512), self._enc_layer(512, 512), self._enc_layer(512, 512))\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True) # 32 -> 16\n        # bottleneck?\n        # self.bottleneck_conv = \n        # https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html        \n        self.enc_conv_bn = nn.Sequential(self._enc_layer(512, 512), self._enc_layer(512, 512), self._enc_layer(512, 512))\n        self.pool_bn = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False, return_indices=True)\n        self.upsample_bn = nn.MaxUnpool2d(kernel_size=2, stride=2, padding=0)\n        self.dec_conv_bn = nn.Sequential(self._dec_layer(512, 512), self._dec_layer(512, 512), self._dec_layer(512, 512))\n        # decoder (upsampling)\n        self.upsample0 = nn.MaxUnpool2d(kernel_size=2, stride=2) # 16 -> 32\n        self.dec_conv0 = nn.Sequential(self._dec_layer(512, 512), self._dec_layer(512, 512), self._dec_layer(512, 256))\n        self.upsample1 = nn.MaxUnpool2d(kernel_size=2, stride=2) # 32 -> 64\n        self.dec_conv1 = nn.Sequential(self._dec_layer(256, 256), self._dec_layer(256, 256), self._dec_layer(256, 128))\n        self.upsample2 = nn.MaxUnpool2d(kernel_size=2, stride=2) # 64 -> 128\n        self.dec_conv2 = nn.Sequential(self._dec_layer(128, 128), self._dec_layer(128, 64))\n        self.upsample3 = nn.MaxUnpool2d(kernel_size=2, stride=2) # 128 -> 256\n        self.dec_conv3 = nn.Sequential(\n            self._dec_layer(64, 64),\n            nn.ConvTranspose2d(64, 1, kernel_size=(3, 3), padding=(1, 1)),\n            # nn.BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n            # nn.ReLU(inplace=True)\n       )\n\n    def forward(self, x):\n        # encoder\n        e0, idx0 = self.pool0(self.enc_conv0(x))\n        e1, idx1 = self.pool1(self.enc_conv1(e0))\n        e2, idx2 = self.pool2(self.enc_conv2(e1))\n        e3, idx3 = self.pool3(self.enc_conv3(e2))\n\n        # bottleneck\n        # b = self.bottleneck_conv(e3)\n        p, idx_bn = self.pool_bn(self.enc_conv_bn(e3))\n        b = self.dec_conv_bn(self.upsample_bn(p, idx_bn))\n        \n        # decoder\n        d0 = self.dec_conv0(self.upsample0(b, idx3))\n        d1 = self.dec_conv1(self.upsample1(d0, idx2))\n        d2 = self.dec_conv2(self.upsample2(d1, idx1))\n        d3 = self.dec_conv3(self.upsample3(d2, idx0)) # no activation\n        return d3","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:23:40.345053Z","iopub.execute_input":"2022-01-29T10:23:40.345356Z","iopub.status.idle":"2022-01-29T10:23:40.369785Z","shell.execute_reply.started":"2022-01-29T10:23:40.345327Z","shell.execute_reply":"2022-01-29T10:23:40.368160Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"segnet = SegNet().to(device)\n# segnet\nsummary(segnet, input_size=(3, 256, 256), batch_size=batch_size)\ndel segnet","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:23:44.428439Z","iopub.execute_input":"2022-01-29T10:23:44.429593Z","iopub.status.idle":"2022-01-29T10:23:44.813612Z","shell.execute_reply.started":"2022-01-29T10:23:44.429532Z","shell.execute_reply":"2022-01-29T10:23:44.812531Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"segnet = SegNet().to(device)\n#optim = torch.optim.Adam(segnet.parameters(), lr=1e-4)\noptim = torch.optim.SGD(segnet.parameters(), lr=0.01, momentum=0.9)\n# loss_func = BCELoss_with_logits()\nloss_func = FocalTverskyLoss(0.6, 0.4, 2)\nlosses, metric = train_model(segnet, loss_func, optim, dataloader_train, dataloader_test, epochs=100)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:23:51.396473Z","iopub.execute_input":"2022-01-29T10:23:51.396829Z","iopub.status.idle":"2022-01-29T10:30:18.133520Z","shell.execute_reply.started":"2022-01-29T10:23:51.396798Z","shell.execute_reply":"2022-01-29T10:30:18.129043Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# score_model(segnet, iou_pytorch, dataloader_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UNet","metadata":{}},{"cell_type":"code","source":"class UNet(nn.Module):\n    \n    def _conv_conv(self,in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n    \n    def _enc_layer(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            self._conv_conv(in_channels, out_channels)\n        )\n    \n    def __init__(self):\n        super().__init__()\n        \n        # encoder (downsampling)\n        # Each enc_conv/dec_conv block should look like this:\n        # nn.Sequential(\n        #     nn.Conv2d(...),\n        #     ... (2 or 3 conv layers with relu and batchnorm),\n        # )\n        \n        self.enc_conv0 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.enc_lr0 = self._enc_layer(64, 128)\n        self.enc_lr1 = self._enc_layer(128, 256)\n        self.enc_lr2 = self._enc_layer(256, 512)\n        self.enc_lr3 = self._enc_layer(512, 512)\n        # decoder (upsampling)\n        self.upsample0 = nn.Upsample(scale_factor=2)#ConvTranspose2d(1024, 512, kernel_size=2, stride=2) # 16 -> 32\n        self.dec_conv0 = self._conv_conv(2 * 512, 256)\n        self.upsample1 = nn.Upsample(scale_factor=2)#nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2) # 32 -> 64\n        self.dec_conv1 = self._conv_conv(2 * 256, 128)\n        self.upsample2 = nn.Upsample(scale_factor=2)#nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) # 64 -> 128\n        self.dec_conv2 = self._conv_conv(2 * 128, 64)\n        self.upsample3 = nn.Upsample(scale_factor=2)#nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)  # 128 -> 256\n        self.dec_conv3 = nn.Sequential(\n            self._conv_conv(2 * 64, 64),\n            nn.Conv2d(64, 1, kernel_size=1),\n        )\n\n    def forward(self, x):\n        # encoder\n        e0 = self.enc_conv0(x)\n        e1 = self.enc_lr0(e0)\n        e2 = self.enc_lr1(e1)\n        e3 = self.enc_lr2(e2)\n        # bottleneck\n        b = self.upsample0(self.enc_lr3(e3))\n\n        # decoder\n        d0 = self.upsample1(self.dec_conv0(torch.cat((b, e3), dim=1)))\n        d1 = self.upsample2(self.dec_conv1(torch.cat((d0, e2), dim=1)))\n        d2 = self.upsample3(self.dec_conv2(torch.cat((d1, e1), dim=1)))\n        d3 = self.dec_conv3(torch.cat((d2, e0), dim=1))  # no activation\n        return d3","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:30:26.309672Z","iopub.execute_input":"2022-01-29T10:30:26.310071Z","iopub.status.idle":"2022-01-29T10:30:26.333667Z","shell.execute_reply.started":"2022-01-29T10:30:26.310024Z","shell.execute_reply":"2022-01-29T10:30:26.332652Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"unet = UNet().to(device)\n# unet\nsummary(unet, input_size=(3, 256, 256), batch_size=batch_size)\ndel unet","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:30:30.375538Z","iopub.execute_input":"2022-01-29T10:30:30.376651Z","iopub.status.idle":"2022-01-29T10:30:30.553883Z","shell.execute_reply.started":"2022-01-29T10:30:30.376593Z","shell.execute_reply":"2022-01-29T10:30:30.552910Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"unet = UNet().to(device)\n#optim = torch.optim.Adam(unet.parameters(), lr=1e-4)\noptim = torch.optim.SGD(unet.parameters(), lr=0.01, momentum=0.9)\nloss_func = LovaszLoss()\nlosses, metric = train_model(unet, loss_func, optim, dataloader_train, dataloader_test, epochs=100)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:30:34.199610Z","iopub.execute_input":"2022-01-29T10:30:34.199923Z","iopub.status.idle":"2022-01-29T10:36:16.553225Z","shell.execute_reply.started":"2022-01-29T10:30:34.199864Z","shell.execute_reply":"2022-01-29T10:36:16.551910Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#score_model(unet, iou_pytorch, dataloader_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNet2(nn.Module):\n    \n    def _conv_conv(self,in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n    \n    def _enc_layer(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=1),\n            self._conv_conv(in_channels, out_channels)\n        )\n    \n    def __init__(self):\n        super().__init__()\n        \n        # encoder (downsampling)\n        # Each enc_conv/dec_conv block should look like this:\n        # nn.Sequential(\n        #     nn.Conv2d(...),\n        #     ... (2 or 3 conv layers with relu and batchnorm),\n        # )\n        \n        self.enc_conv0 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.enc_lr0 = self._enc_layer(64, 128)\n        self.enc_lr1 = self._enc_layer(128, 256)\n        self.enc_lr2 = self._enc_layer(256, 512)\n        self.enc_lr3 = self._enc_layer(512, 1024)\n        # decoder (upsampling)\n        self.upsample0 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2) # 16 -> 32\n        self.dec_conv0 = self._conv_conv(2 * 512, 512)\n        self.upsample1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2) # 32 -> 64\n        self.dec_conv1 = self._conv_conv(2 * 256, 256)\n        self.upsample2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) # 64 -> 128\n        self.dec_conv2 = self._conv_conv(2 * 128, 128)\n        self.upsample3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)  # 128 -> 256\n        self.dec_conv3 = nn.Sequential(\n            self._conv_conv(2 * 64, 64),\n            nn.Conv2d(64, 1, kernel_size=1),\n        )\n\n    def forward(self, x):\n        # encoder\n        e0 = self.enc_conv0(x)\n        e1 = self.enc_lr0(e0)\n        e2 = self.enc_lr1(e1)\n        e3 = self.enc_lr2(e2)\n        # bottleneck\n        b = self.upsample0(self.enc_lr3(e3))\n\n        # decoder\n        d0 = self.upsample1(self.dec_conv0(torch.cat((b, e3), dim=1)))\n        d1 = self.upsample2(self.dec_conv1(torch.cat((d0, e2), dim=1)))\n        d2 = self.upsample3(self.dec_conv2(torch.cat((d1, e1), dim=1)))\n        d3 = self.dec_conv3(torch.cat((d2, e0), dim=1))  # no activation\n        return d3","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-29T10:37:08.536299Z","iopub.execute_input":"2022-01-29T10:37:08.536617Z","iopub.status.idle":"2022-01-29T10:37:08.558612Z","shell.execute_reply.started":"2022-01-29T10:37:08.536571Z","shell.execute_reply":"2022-01-29T10:37:08.557203Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"unet2 = UNet2().to(device)\n# unet2\nsummary(unet2, input_size=(3, 256, 256), batch_size=batch_size)\ndel unet2","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:37:14.421220Z","iopub.execute_input":"2022-01-29T10:37:14.421508Z","iopub.status.idle":"2022-01-29T10:37:14.805965Z","shell.execute_reply.started":"2022-01-29T10:37:14.421476Z","shell.execute_reply":"2022-01-29T10:37:14.804881Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"unet2 = UNet2().to(device)\n#optim = torch.optim.Adam(unet2.parameters(), lr=1e-4)\noptim = torch.optim.SGD(unet2.parameters(), lr=0.01, momentum=0.9)\nloss_func = LovaszLoss()\nlosses, metric = train_model(unet2, loss_func, optim, dataloader_train, dataloader_test, epochs=100)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:38:09.750155Z","iopub.execute_input":"2022-01-29T10:38:09.750613Z","iopub.status.idle":"2022-01-29T10:46:16.292841Z","shell.execute_reply.started":"2022-01-29T10:38:09.750556Z","shell.execute_reply":"2022-01-29T10:46:16.291939Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Model validation","metadata":{}},{"cell_type":"code","source":"def validate_model(model_class, model_file, pickle_file, max_epochs=100):\n    loss = {\n        #'bce': BCELoss_with_logits(reduction='mean', truncate=True),\n        'bce': BCELoss_classic(reduction='mean'),\n        #'bce': nn.BCEWithLogitsLoss(reduction='mean'),\n        'dice': DiceLoss(),\n        #'focal': FocalLoss(),\n        'tversky': TverskyLoss(alpha=0.7, beta = 0.3),\n        'focal_tversky': FocalTverskyLoss(alpha=0.6, beta = 0.4, gamma=2),\n        'lovasz': LovaszLoss(),\n    }\n    model_history = {}\n    for loss_name, loss_func in loss.items():\n        model_history[loss_name] = {}\n        model = model_class().to(device)\n        #optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n        optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n        losses, metric = train_model(model, loss_func, optim, dataloader_train, dataloader_test, max_epochs)\n        model_history[loss_name]['losses'] = losses\n        model_history[loss_name]['metric'] = metric\n        torch.save(model, '{}_{}_{}epoch.model'.format(model_file, loss_name, max_epochs))\n    with open(pickle_file, 'wb') as handle:\n        pickle.dump(model_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    return model_history","metadata":{"execution":{"iopub.status.busy":"2022-01-29T09:23:09.733756Z","iopub.execute_input":"2022-01-29T09:23:09.734507Z","iopub.status.idle":"2022-01-29T09:23:09.743866Z","shell.execute_reply.started":"2022-01-29T09:23:09.734462Z","shell.execute_reply":"2022-01-29T09:23:09.743002Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"segnet","metadata":{}},{"cell_type":"code","source":"max_epochs = 100\nsegnet_history = validate_model(SegNet, 'segnet', 'segnet.pickle', max_epochs=max_epochs)\n#unet_history = validate_model(UNet, 'unet', 'unet.pickle', max_epochs=max_epochs)\n#unet2_history = validate_model(UNet2, 'unet2', 'unet2.pickle', max_epochs=max_epochs)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T09:23:18.297792Z","iopub.execute_input":"2022-01-29T09:23:18.298038Z","iopub.status.idle":"2022-01-29T09:54:06.501709Z","shell.execute_reply.started":"2022-01-29T09:23:18.298011Z","shell.execute_reply":"2022-01-29T09:54:06.500957Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# with open('../input/temporary/segnet.pickle', 'rb') as handle:\n#     segnet_history = pickle.load(handle)\n# with open('../input/temporary/unet.pickle', 'rb') as handle:\n#     unet_history = pickle.load(handle)\n# with open('../input/temporary/unet2.pickle', 'rb') as handle:\n#     unet2_history = pickle.load(handle)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.set(context='paper')\n\nfor history in [segnet_history]:\n    fig, ax = plt.subplots(1, 2, figsize=(12,4))\n    x = list(range(max_epochs))\n    for key, value in history.items():\n        sns.lineplot(x=x, y=value['losses'], ax=ax[0], label=key)\n        sns.lineplot(x=x, y=value['metric'], ax=ax[1], label=key)\n    ax[0].set_title('loss')\n    ax[1].set_title('IoU')\n    ax[0].legend()\n    ax[1].legend()\n    fig.show()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-29T09:55:09.496674Z","iopub.execute_input":"2022-01-29T09:55:09.496937Z","iopub.status.idle":"2022-01-29T09:55:10.258639Z","shell.execute_reply.started":"2022-01-29T09:55:09.496908Z","shell.execute_reply":"2022-01-29T09:55:10.257950Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"for history in [segnet_history]:\n    ax = sns.barplot(x=list(history.keys()), y=[max(m['metric']) for m in history.values()])\n    ax.bar_label(ax.containers[0])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T09:55:18.822762Z","iopub.execute_input":"2022-01-29T09:55:18.823024Z","iopub.status.idle":"2022-01-29T09:55:19.047206Z","shell.execute_reply.started":"2022-01-29T09:55:18.822995Z","shell.execute_reply":"2022-01-29T09:55:19.046496Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"unet","metadata":{}},{"cell_type":"code","source":"def validate_model(model_class, model_file, pickle_file, max_epochs=100):\n    loss = {\n        #'bce': BCELoss_with_logits(reduction='mean', truncate=True),\n        'bce': BCELoss_classic(reduction='mean'),\n        #'bce': nn.BCEWithLogitsLoss(reduction='mean'),\n        'dice': DiceLoss(),\n        'focal': FocalLoss(),\n        'tversky': TverskyLoss(alpha=0.7, beta = 0.3),\n        'focal_tversky': FocalTverskyLoss(alpha=0.6, beta = 0.4, gamma=2),\n        'lovasz': LovaszLoss(),\n    }\n    model_history = {}\n    for loss_name, loss_func in loss.items():\n        model_history[loss_name] = {}\n        model = model_class().to(device)\n        #optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n        optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n        losses, metric = train_model(model, loss_func, optim, dataloader_train, dataloader_test, max_epochs)\n        model_history[loss_name]['losses'] = losses\n        model_history[loss_name]['metric'] = metric\n        torch.save(model, '{}_{}_{}epoch.model'.format(model_file, loss_name, max_epochs))\n    with open(pickle_file, 'wb') as handle:\n        pickle.dump(model_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    return model_history","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:48:08.243700Z","iopub.execute_input":"2022-01-29T10:48:08.244297Z","iopub.status.idle":"2022-01-29T10:48:08.254481Z","shell.execute_reply.started":"2022-01-29T10:48:08.244247Z","shell.execute_reply":"2022-01-29T10:48:08.253212Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"max_epochs = 100\n#segnet_history = validate_model(SegNet, 'segnet', 'segnet.pickle', max_epochs=max_epochs)\nunet_history = validate_model(UNet, 'unet', 'unet.pickle', max_epochs=max_epochs)\n#unet2_history = validate_model(UNet2, 'unet2', 'unet2.pickle', max_epochs=max_epochs)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:48:22.125479Z","iopub.execute_input":"2022-01-29T10:48:22.125989Z","iopub.status.idle":"2022-01-29T11:21:47.701767Z","shell.execute_reply.started":"2022-01-29T10:48:22.125944Z","shell.execute_reply":"2022-01-29T11:21:47.700789Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.set(context='paper')\n\nfor history in [unet_history]:\n    fig, ax = plt.subplots(1, 2, figsize=(12,4))\n    x = list(range(max_epochs))\n    for key, value in history.items():\n        sns.lineplot(x=x, y=value['losses'], ax=ax[0], label=key)\n        sns.lineplot(x=x, y=value['metric'], ax=ax[1], label=key)\n    ax[0].set_title('loss')\n    ax[1].set_title('IoU')\n    ax[0].legend()\n    ax[1].legend()\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:22:31.693212Z","iopub.execute_input":"2022-01-29T11:22:31.693545Z","iopub.status.idle":"2022-01-29T11:22:32.815322Z","shell.execute_reply.started":"2022-01-29T11:22:31.693512Z","shell.execute_reply":"2022-01-29T11:22:32.814403Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"for history in [unet_history,]:\n    ax = sns.barplot(x=list(history.keys()), y=[max(m['metric']) for m in history.values()])\n    ax.bar_label(ax.containers[0])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:22:40.863828Z","iopub.execute_input":"2022-01-29T11:22:40.864131Z","iopub.status.idle":"2022-01-29T11:22:41.170644Z","shell.execute_reply.started":"2022-01-29T11:22:40.864100Z","shell.execute_reply":"2022-01-29T11:22:41.169731Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"unet2","metadata":{}},{"cell_type":"code","source":"max_epochs = 100\n#segnet_history = validate_model(SegNet, 'segnet', 'segnet.pickle', max_epochs=max_epochs)\n#unet_history = validate_model(UNet, 'unet', 'unet.pickle', max_epochs=max_epochs)\nunet2_history = validate_model(UNet2, 'unet2', 'unet2.pickle', max_epochs=max_epochs)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:24:12.479669Z","iopub.execute_input":"2022-01-29T11:24:12.480825Z","iopub.status.idle":"2022-01-29T12:15:21.003056Z","shell.execute_reply.started":"2022-01-29T11:24:12.480773Z","shell.execute_reply":"2022-01-29T12:15:21.001988Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.set(context='paper')\n\nfor history in [unet2_history]:\n    fig, ax = plt.subplots(1, 2, figsize=(12,4))\n    x = list(range(max_epochs))\n    for key, value in history.items():\n        sns.lineplot(x=x, y=value['losses'], ax=ax[0], label=key)\n        sns.lineplot(x=x, y=value['metric'], ax=ax[1], label=key)\n    ax[0].set_title('loss')\n    ax[1].set_title('IoU')\n    ax[0].legend()\n    ax[1].legend()\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T12:15:35.228491Z","iopub.execute_input":"2022-01-29T12:15:35.228794Z","iopub.status.idle":"2022-01-29T12:15:36.279432Z","shell.execute_reply.started":"2022-01-29T12:15:35.228745Z","shell.execute_reply":"2022-01-29T12:15:36.278262Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"for history in [unet2_history,]:\n    ax = sns.barplot(x=list(history.keys()), y=[max(m['metric']) for m in history.values()])\n    ax.bar_label(ax.containers[0])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T12:15:47.051315Z","iopub.execute_input":"2022-01-29T12:15:47.051648Z","iopub.status.idle":"2022-01-29T12:15:47.366795Z","shell.execute_reply.started":"2022-01-29T12:15:47.051617Z","shell.execute_reply":"2022-01-29T12:15:47.365859Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# Resume","metadata":{}},{"cell_type":"markdown","source":"**SegNet**\n\n* Total params: 29,443,585\n* Params size (MB): 112.32\n   \n**Unet (MaxPool2d/Upsample)**\n\n* Total params: 13,387,393\n* Params size (MB): 51.07\n\n**UNet (Conv2d/ConvTranspose2d)**\n\n* Total params: 34,166,145\n* Params size (MB): 130.33","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary gdown\n#!pip install albumentations==0.4.6","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:10.665888Z","iopub.execute_input":"2022-03-01T21:41:10.666146Z","iopub.status.idle":"2022-03-01T21:41:32.676586Z","shell.execute_reply.started":"2022-03-01T21:41:10.666067Z","shell.execute_reply":"2022-03-01T21:41:32.675774Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#import warnings\n#warnings.simplefilter('error', UserWarning)\n\n#from IPython.core.interactiveshell import InteractiveShell\n#InteractiveShell.ast_node_interactivity = \"all\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom skimage import io\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as tfs\nimport torchvision.models\nfrom torchsummary import summary\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nfrom IPython.display import clear_output\nfrom tqdm.notebook import tqdm\nimport pickle\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:32.678433Z","iopub.execute_input":"2022-03-01T21:41:32.678722Z","iopub.status.idle":"2022-03-01T21:41:36.940774Z","shell.execute_reply.started":"2022-03-01T21:41:32.678683Z","shell.execute_reply":"2022-03-01T21:41:36.940015Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ncontent_dir = '/kaggle/working/PH2Dataset'\nbatch_size = 16","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:36.942126Z","iopub.execute_input":"2022-03-01T21:41:36.942380Z","iopub.status.idle":"2022-03-01T21:41:36.998693Z","shell.execute_reply.started":"2022-03-01T21:41:36.942346Z","shell.execute_reply":"2022-03-01T21:41:36.997598Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data","metadata":{}},{"cell_type":"code","source":"if not os.path.exists(content_dir):\n    print('Download dataset...')\n    # !wget --quiet https://www.dropbox.com/s/k88qukc20ljnbuo/PH2Dataset.rar\n    !gdown https://drive.google.com/uc?id=1JHRw-LcPoHoApGNoub2b-DgrSWnRw-ZB -O PH2Dataset.zip\n    # print('Install unrar...', end='')\n    # !apt-get -qq install unrar > /dev/null 2>&1\n    # print('done.')\n    print('Extract archive files...', end='')\n    #!unrar x -idq PH2Dataset.rar\n    !unzip -q PH2Dataset.zip\n    print('done.')","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:37.001424Z","iopub.execute_input":"2022-03-01T21:41:37.002809Z","iopub.status.idle":"2022-03-01T21:41:41.327240Z","shell.execute_reply.started":"2022-03-01T21:41:37.002768Z","shell.execute_reply":"2022-03-01T21:41:41.326250Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class CustomDataSet(Dataset):\n    def __init__(self, main_dir, transform):\n        self.transform = transform\n        self.samples = []\n        self.labels = []\n        for root, dirs, files in os.walk(main_dir):\n            if root.endswith('_Dermoscopic_Image'):\n                self.samples.append(os.path.join(root, files[0]))\n            if root.endswith('_lesion'):\n                self.labels.append(os.path.join(root, files[0]))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        # image = Image.open(self.samples[idx])\n        # label = Image.open(self.labels[idx])\n        image = cv2.imread(self.samples[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = cv2.imread(self.labels[idx], cv2.IMREAD_GRAYSCALE)\n        transformed = self.transform(image=image, mask=label)\n        tensor_image = transformed['image'].transpose(2, 0, 1)\n        label_image = transformed['mask'][np.newaxis, :]\n        tensor_image = torch.FloatTensor(tensor_image) / 255\n        label_image = torch.FloatTensor(label_image) / 255\n        #print(tensor_image.shape, label_image.shape)\n        #print(tensor_image.min(), tensor_image.max(), label_image.min(), label_image.max())\n        return tensor_image, label_image","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:41.329282Z","iopub.execute_input":"2022-03-01T21:41:41.329577Z","iopub.status.idle":"2022-03-01T21:41:41.339490Z","shell.execute_reply.started":"2022-03-01T21:41:41.329537Z","shell.execute_reply":"2022-03-01T21:41:41.338806Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"transform = A.Compose([\n    A.Resize(256, 256),\n#     A.HorizontalFlip(),\n#     A.VerticalFlip(),\n#     A.augmentations.geometric.Affine(\n#         scale={'x':(1.0, 1.2), 'y':(1.0, 1.2)},\n# #        cval=[55, 51, 49],\n# #        mode=cv2.BORDER_CONSTANT\n#     )\n    #A.ToFloat(),\n    #A.pytorch.ToTensorV2(),\n    #A.Normalize(mean=0, std=1),\n    #tfs.RandomAffine(10,translate=(0.1, 0.1), scale=(0.9, 1.1)),\n])\n\n# transform = tfs.Compose([\n#     tfs.Resize((256, 256)),\n#     tfs.ToTensor(),\n# ])\n\ndermoscopic_dataset = CustomDataSet(os.path.join(content_dir, 'PH2 Dataset images'), transform=transform)\n\nidx = np.random.choice(len(dermoscopic_dataset), len(dermoscopic_dataset), False)\n#train_idx, valid_idx, test_idx = np.split(idx, [50, 70])\ntrain_idx, test_idx = np.split(idx, [70])\n\ndataset_train = Subset(dermoscopic_dataset, train_idx)\n#dataset_valid = Subset(dermoscopic_dataset, valid_idx)\ndataset_test  = Subset(dermoscopic_dataset, test_idx)\n\ndataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n#dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=True)\ndataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:41.340970Z","iopub.execute_input":"2022-03-01T21:41:41.341239Z","iopub.status.idle":"2022-03-01T21:41:41.364968Z","shell.execute_reply.started":"2022-03-01T21:41:41.341202Z","shell.execute_reply":"2022-03-01T21:41:41.364246Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"def show_dermoscopic_imgs(images, labels, threshold=None):\n    images = images.numpy().transpose(0, 2, 3, 1)\n    labels = labels.numpy().transpose(0, 2, 3, 1)\n    if threshold is not None:\n        labels = np.where(labels > threshold, 1, 0)\n    plt.figure(figsize=(18, 6))\n    for i in range(4):\n        plt.subplot(2, 6, i+1)\n        plt.imshow(images[i])\n        plt.axis(\"off\")\n\n        plt.subplot(2, 6, i+7)\n        plt.imshow(labels[i], cmap='gray')\n        plt.axis(\"off\")\n    # plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:41.366027Z","iopub.execute_input":"2022-03-01T21:41:41.366279Z","iopub.status.idle":"2022-03-01T21:41:41.375311Z","shell.execute_reply.started":"2022-03-01T21:41:41.366244Z","shell.execute_reply":"2022-03-01T21:41:41.374375Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"images, labels = next(iter(dataloader_train))\nshow_dermoscopic_imgs(images, labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:41.376454Z","iopub.execute_input":"2022-03-01T21:41:41.377239Z","iopub.status.idle":"2022-03-01T21:41:42.230014Z","shell.execute_reply.started":"2022-03-01T21:41:41.377175Z","shell.execute_reply":"2022-03-01T21:41:42.228644Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def show_loss(history):\n    plt.figure(figsize=(12, 8))\n    plt.plot(history)\n    plt.title('Loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.231034Z","iopub.execute_input":"2022-03-01T21:41:42.231269Z","iopub.status.idle":"2022-03-01T21:41:42.237402Z","shell.execute_reply.started":"2022-03-01T21:41:42.231237Z","shell.execute_reply":"2022-03-01T21:41:42.236542Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Metrics\n\nGood summary: [A survey of loss functions for semantic segmentation](https://arxiv.org/pdf/2006.14822.pdf)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:13:39.409414Z","iopub.execute_input":"2021-11-18T13:13:39.409711Z","iopub.status.idle":"2021-11-18T13:13:39.413556Z","shell.execute_reply.started":"2021-11-18T13:13:39.409677Z","shell.execute_reply":"2021-11-18T13:13:39.412683Z"}}},{"cell_type":"markdown","source":"## IoU scoring metric\n\n$$I o U=\\frac{\\text {target } \\cap \\text { prediction }}{\\text {target } \\cup{prediction }}$$","metadata":{}},{"cell_type":"code","source":"SMOOTH = 1e-6\n# https://www.kaggle.com/iezepov/fast-iou-scoring-metric-in-pytorch-and-numpy\ndef iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n    outputs = outputs.byte()\n    labels = labels.byte()\n    #print(outputs)\n    # You can comment out this line if you are passing tensors of equal shape\n    # But if you are passing output from UNet or something it will most probably\n    # be with the BATCH x 1 x H x W shape\n    #outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n    # thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n    # return thresholded  # Or thresholded.mean() if you are interested in average across the batch\n    return iou","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.241559Z","iopub.execute_input":"2022-03-01T21:41:42.241844Z","iopub.status.idle":"2022-03-01T21:41:42.249377Z","shell.execute_reply.started":"2022-03-01T21:41:42.241808Z","shell.execute_reply":"2022-03-01T21:41:42.248697Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## BCE Loss\n\n$$\\mathcal L_{BCE}(y, \\hat y) = -\\sum_i \\left[y_i\\log\\sigma(\\hat y_i) + (1-y_i)\\log(1-\\sigma(\\hat y_i))\\right]$$","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:27:55.204072Z","iopub.execute_input":"2021-11-18T13:27:55.204598Z","iopub.status.idle":"2021-11-18T13:27:55.207505Z","shell.execute_reply.started":"2021-11-18T13:27:55.204565Z","shell.execute_reply":"2021-11-18T13:27:55.206925Z"}}},{"cell_type":"code","source":"class BCELoss_classic(nn.Module):\n    def __init__(self, reduction='mean'):\n        super().__init__()\n        if reduction not in ('mean', 'sum'):\n            raise ValueError('\"{}\" is not a valid mode for reduction. Only \"mean\"'\n                             'and \"sum\" are allowed.'.format(rediction))\n        self.reduction = reduction\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        # outputs = torch.clamp(outputs, 0, 1)\n        outputs = torch.sigmoid(outputs)\n        bce = labels * torch.log(outputs + SMOOTH) + (1 - labels) * torch.log(1 - outputs + SMOOTH)\n        if self.reduction == 'mean':\n            return -torch.mean(bce)\n        elif self.reduction == 'sum':\n            return -torch.sum(bce)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.250927Z","iopub.execute_input":"2022-03-01T21:41:42.251428Z","iopub.status.idle":"2022-03-01T21:41:42.262457Z","shell.execute_reply.started":"2022-03-01T21:41:42.251389Z","shell.execute_reply":"2022-03-01T21:41:42.261674Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"$$\\mathcal L_{BCE}(y, \\hat y) = \\hat y - y\\hat y + \\log\\left(1+\\exp(-\\hat y)\\right)$$","metadata":{}},{"cell_type":"code","source":"class BCELoss_with_logits(nn.Module):\n    def __init__(self, reduction='mean', truncate=False):\n        super().__init__()\n        if reduction not in ('mean', 'sum'):\n            raise ValueError('\"{}\" is not a valid mode for reduction. Only \"mean\"'\n                             'and \"sum\" are allowed.'.format(rediction))\n        self.reduction = reduction\n        self.truncate = truncate\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        # classical without sigmoid\n        # but you need to cut off large negative logits, otherwise it will be -inf -> nan \n        if self.truncate:\n            outputs = torch.sigmoid(outputs)\n        outputs = outputs.float()\n        labels = labels.float()\n        # print(torch.min(outputs))\n        bce = outputs - labels * outputs + torch.log(1 + torch.exp(-outputs))\n        if self.reduction == 'mean':\n            return torch.mean(bce)\n        elif self.reduction == 'sum':\n            return torch.sum(bce)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.263546Z","iopub.execute_input":"2022-03-01T21:41:42.263876Z","iopub.status.idle":"2022-03-01T21:41:42.273266Z","shell.execute_reply.started":"2022-03-01T21:41:42.263839Z","shell.execute_reply":"2022-03-01T21:41:42.272447Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# torch.mean(torch.log(1 + torch.exp(torch.tensor(255))))","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.274325Z","iopub.execute_input":"2022-03-01T21:41:42.274654Z","iopub.status.idle":"2022-03-01T21:41:42.285349Z","shell.execute_reply.started":"2022-03-01T21:41:42.274585Z","shell.execute_reply":"2022-03-01T21:41:42.284662Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"output = torch.Tensor([\n    [-0.4717,  0.8484,  0.7424],\n    [ 0.0880,  0.1379,  0.8387],\n    [ 0.3874, -1.8205,  1.5422]\n])\ntarget = torch.Tensor([\n    [0, 1, 0],\n    [1, 0, 1],\n    [0, 1, 0]\n])\nsigm = nn.Sigmoid()\n\nprint(\n    output,\n    target,\n    nn.BCELoss(reduction='mean')(sigm(output), target),\n    BCELoss_classic(reduction='mean')(output, target),\n    BCELoss_with_logits(reduction='mean')(output, target),\n    sep='\\n'\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.286505Z","iopub.execute_input":"2022-03-01T21:41:42.287183Z","iopub.status.idle":"2022-03-01T21:41:42.380352Z","shell.execute_reply.started":"2022-03-01T21:41:42.287135Z","shell.execute_reply":"2022-03-01T21:41:42.379552Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Dice Loss\n\n$$D(X,Y)=\\frac{2|X\\cap Y|}{|X|+|Y|}$$\n\n$$\\mathcal L_D(X,Y) = 1-\\frac{1}{256 \\times 256} \\times \\sum_i\\frac{2X_iY_i}{X_i+Y_i}.$$","metadata":{}},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, reduction='mean'):\n        super().__init__()\n\n#     # not work, why?\n#     def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n#         coef = 1 / (outputs.shape[0] * outputs.shape[2] * outputs.shape[3])\n#         outputs = torch.sigmoid(outputs)\n#         num = 2 * outputs * labels\n#         den = outputs + labels\n#         res = 1 - coef * ((num + 1)/(den + 1)).sum()\n#         return res\n\n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        outputs = torch.sigmoid(outputs)\n        num = 2 * (outputs * labels).sum()\n        den = (outputs + labels).sum()\n        res = 1 - (num + SMOOTH) / (den + SMOOTH)\n        return res","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.381720Z","iopub.execute_input":"2022-03-01T21:41:42.382052Z","iopub.status.idle":"2022-03-01T21:41:42.389557Z","shell.execute_reply.started":"2022-03-01T21:41:42.382011Z","shell.execute_reply":"2022-03-01T21:41:42.388673Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Focal Loss\n\n$$FL(p_t) = -\\alpha_t(1-p_t)^\\gamma log(p_t)$$\n$$CE(p,y) = CE(p_t) = -log(p_t)$$\n$$p_t = e^{-CE(p_t)}$$\n$$FL(p_t) = \\alpha_t(1-e^{-CE(p_t)})^\\gamma CE(p_t)$$","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:15:44.629785Z","iopub.execute_input":"2021-11-21T09:15:44.630127Z","iopub.status.idle":"2021-11-21T09:15:44.635131Z","shell.execute_reply.started":"2021-11-21T09:15:44.630085Z","shell.execute_reply":"2021-11-21T09:15:44.634061Z"}}},{"cell_type":"code","source":"# https://arxiv.org/pdf/1708.02002.pdf\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha: int = 1, gamma: int = 2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        bce_logit = nn.BCEWithLogitsLoss()\n        ce = bce_logit(outputs, labels)\n        pt = torch.exp(-ce)\n        fl = self.alpha * torch.pow((1 - pt), self.gamma) * ce\n        return fl","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.391157Z","iopub.execute_input":"2022-03-01T21:41:42.391658Z","iopub.status.idle":"2022-03-01T21:41:42.401959Z","shell.execute_reply.started":"2022-03-01T21:41:42.391605Z","shell.execute_reply":"2022-03-01T21:41:42.401183Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Tversky Loss\n\n[Tversky loss function for image segmentation using 3D fully convolutional deep networks](https://arxiv.org/abs/1706.05721)\n\n$$TI(p,\\hat p) = \\frac{p\\hat p}{p\\hat p + \\beta(1 − p)\\hat p + (1 − \\beta)p(1 − \\hat p)}$$\n$$TL(p,\\hat p) = 1 - \\frac{1 + p\\hat p}{1+ p\\hat p + \\beta(1 − p)\\hat p + (1 − \\beta)p(1 − \\hat p)}$$","metadata":{}},{"cell_type":"code","source":"class TverskyLoss(nn.Module):\n    def __init__(self, alpha:int = 0.5, beta:int = 0.5):\n        super().__init__()\n        self.alpha = alpha\n        self.beta = beta\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        outputs = torch.sigmoid(outputs)\n        pp = (labels * outputs).sum()\n        den1 = self.alpha * ((1 - labels) * outputs).sum()\n        den2 = self.beta * (labels * (1 - outputs)).sum()\n        tl = 1 - (1 + pp) / (1 + pp + den1 + den2)\n        return tl      ","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.403954Z","iopub.execute_input":"2022-03-01T21:41:42.404327Z","iopub.status.idle":"2022-03-01T21:41:42.413919Z","shell.execute_reply.started":"2022-03-01T21:41:42.404288Z","shell.execute_reply":"2022-03-01T21:41:42.413132Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Focal Tversky Loss","metadata":{}},{"cell_type":"code","source":"class FocalTverskyLoss(nn.Module):\n    def __init__(self, alpha:int = 0.5, beta:int = 0.5, gamma:int = 2):\n        super().__init__()\n        self.gamma = gamma\n        self.tl = TverskyLoss(alpha, beta)\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        tl = self.tl(outputs, labels)\n        return torch.pow(tl, self.gamma)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.415461Z","iopub.execute_input":"2022-03-01T21:41:42.416222Z","iopub.status.idle":"2022-03-01T21:41:42.423203Z","shell.execute_reply.started":"2022-03-01T21:41:42.416117Z","shell.execute_reply":"2022-03-01T21:41:42.422455Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Lovasz Loss","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/67791\nclass LovaszLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        #outputs = torch.sigmoid(outputs)\n        outputs = outputs.flatten()\n        labels = labels.flatten()\n        signs = 2 * labels.float() - 1\n        errors = (1 - outputs * signs)\n        errors_sorted, indices = torch.sort(errors, dim=0, descending=True)\n        gt_sorted = labels[indices.data]\n\n        # gradient\n        gts = gt_sorted.sum()\n        intersection = gts - gt_sorted.float().cumsum(0)\n        union = gts + (1 - gt_sorted).float().cumsum(0)\n        grad = 1. - intersection / union\n\n        p = len(gt_sorted)\n        grad[1:p] = grad[1:p] - grad[0:-1]\n       \n        loss = torch.dot(torch.relu(errors_sorted), grad)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.424097Z","iopub.execute_input":"2022-03-01T21:41:42.425106Z","iopub.status.idle":"2022-03-01T21:41:42.436952Z","shell.execute_reply.started":"2022-03-01T21:41:42.425067Z","shell.execute_reply":"2022-03-01T21:41:42.436140Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Train/valid functions","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:33:36.596973Z","iopub.execute_input":"2021-11-21T13:33:36.598099Z","iopub.status.idle":"2021-11-21T13:33:36.625852Z","shell.execute_reply.started":"2021-11-21T13:33:36.597927Z","shell.execute_reply":"2021-11-21T13:33:36.624662Z"}}},{"cell_type":"code","source":"def score_model(model, metric, data, threshold=0):\n    model.to(device).eval() # testing mode\n    scores = 0\n    threshold = torch.tensor(threshold).to(device)\n    for X_batch, Y_label in data:\n        X_batch = X_batch.to(device)\n        with torch.no_grad():\n            Y_pred = model(X_batch)\n        scores += metric((Y_pred > threshold), Y_label.to(device)).mean().item()\n    return scores/len(data)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.438268Z","iopub.execute_input":"2022-03-01T21:41:42.438539Z","iopub.status.idle":"2022-03-01T21:41:42.447050Z","shell.execute_reply.started":"2022-03-01T21:41:42.438502Z","shell.execute_reply":"2022-03-01T21:41:42.446244Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def train_model(\n    model: torch.nn.Module,\n    criterion: torch.nn.Module,\n    optimizer: torch.nn.Module,\n    dataloader_train: torch.utils.data.DataLoader,\n    dataloader_test: torch.utils.data.DataLoader,\n    epochs: int = 100\n) -> (torch.nn.Module, dict):\n    r\"\"\"Training the model. Returns list of train losses.\n    Args:\n        model (torch.nn.Module): Neural network\n        criterion (torch.nn.Module): Cost function\n        optimizer (torch.nn.Module): Optimization algorithm\n        dataloader_train: (torch.utils.data.DataLoader): Train data\n        dataloader_valid: (torch.utils.data.DataLoader): Valid data\n        epochs (int): Number of training iterations. Default: 20\n    \"\"\"\n#def train(model, optimizer, loss_fn, epochs, data_tr, data_val):\n    X_val, Y_val = next(iter(dataloader_test))\n    losses = []\n    metric = []\n  \n    for epoch in range(epochs):\n        avg_loss = 0\n        model.train()  # train mode\n        for X_batch, Y_batch in tqdm(dataloader_train, desc='Progress'):\n            # data to device\n            X_batch = X_batch.to(device)\n            Y_batch = Y_batch.to(device)\n            # set parameter gradients to zero\n            optimizer.zero_grad()\n            # forward\n            # print(Y_batch.shape)\n            Y_pred = model(X_batch)\n            loss = criterion(Y_pred, Y_batch)\n            print(loss)\n            loss.backward() # backward-pass\n            optimizer.step()  # update weights\n            # calculate loss to show the user\n            avg_loss += loss / len(dataloader_train)\n\n        losses.append(avg_loss.item())\n        metric.append(score_model(model, iou_pytorch, dataloader_test))\n        \n        # show intermediate results\n        model.eval()  # testing mode\n        # detach and put into cpu\n        Y_hat = model(X_val.to(device)).detach().cpu()\n        # Visualize tools\n        clear_output(wait=True)\n        show_dermoscopic_imgs(X_val, Y_hat, threshold=0.1)\n        plt.title('%d / %d - loss: %f' % (epoch+1, epochs, avg_loss))\n        plt.show()\n    return losses, metric","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.448517Z","iopub.execute_input":"2022-03-01T21:41:42.448805Z","iopub.status.idle":"2022-03-01T21:41:42.461346Z","shell.execute_reply.started":"2022-03-01T21:41:42.448768Z","shell.execute_reply":"2022-03-01T21:41:42.460633Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def predict(model, data):\n    model.to(device).eval()  # testing mode\n    with torch.no_grad():\n        Y_pred = [model(X_batch.to(device)) for X_batch, _ in data]\n    return Y_pred","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.462408Z","iopub.execute_input":"2022-03-01T21:41:42.463091Z","iopub.status.idle":"2022-03-01T21:41:42.474385Z","shell.execute_reply.started":"2022-03-01T21:41:42.463065Z","shell.execute_reply":"2022-03-01T21:41:42.473661Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# SegNet","metadata":{}},{"cell_type":"code","source":"# vgg16 = torchvision.models.vgg16_bn()\n# vgg16.features","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.475514Z","iopub.execute_input":"2022-03-01T21:41:42.476333Z","iopub.status.idle":"2022-03-01T21:41:42.486867Z","shell.execute_reply.started":"2022-03-01T21:41:42.476294Z","shell.execute_reply":"2022-03-01T21:41:42.486054Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class SegNet(nn.Module):\n    def _enc_layer(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n    \n    def _dec_layer(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n  \n    def __init__(self):\n        super().__init__()\n\n        # encoder (downsampling)\n        # Each enc_conv/dec_conv block should look like this:\n        # nn.Sequential(\n        #     nn.Conv2d(...),\n        #     ... (2 or 3 conv layers with relu and batchnorm),\n        # )\n\n        self.enc_conv0 = nn.Sequential(self._enc_layer(3, 64), self._enc_layer(64, 64))\n        self.pool0 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True) # 256 -> 128\n        self.enc_conv1 = nn.Sequential(self._enc_layer(64, 128), self._enc_layer(128, 128))\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True) # 128 -> 64\n        self.enc_conv2 = nn.Sequential(self._enc_layer(128, 256), self._enc_layer(256, 256), self._enc_layer(256, 256))\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True) # 64 -> 32\n        self.enc_conv3 = nn.Sequential(self._enc_layer(256, 512), self._enc_layer(512, 512), self._enc_layer(512, 512))\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True) # 32 -> 16\n        # bottleneck?\n        # self.bottleneck_conv = \n        # https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html        \n        self.enc_conv_bn = nn.Sequential(self._enc_layer(512, 512), self._enc_layer(512, 512), self._enc_layer(512, 512))\n        self.pool_bn = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False, return_indices=True)\n        self.upsample_bn = nn.MaxUnpool2d(kernel_size=2, stride=2, padding=0)\n        self.dec_conv_bn = nn.Sequential(self._dec_layer(512, 512), self._dec_layer(512, 512), self._dec_layer(512, 512))\n        # decoder (upsampling)\n        self.upsample0 = nn.MaxUnpool2d(kernel_size=2, stride=2) # 16 -> 32\n        self.dec_conv0 = nn.Sequential(self._dec_layer(512, 512), self._dec_layer(512, 512), self._dec_layer(512, 256))\n        self.upsample1 = nn.MaxUnpool2d(kernel_size=2, stride=2) # 32 -> 64\n        self.dec_conv1 = nn.Sequential(self._dec_layer(256, 256), self._dec_layer(256, 256), self._dec_layer(256, 128))\n        self.upsample2 = nn.MaxUnpool2d(kernel_size=2, stride=2) # 64 -> 128\n        self.dec_conv2 = nn.Sequential(self._dec_layer(128, 128), self._dec_layer(128, 64))\n        self.upsample3 = nn.MaxUnpool2d(kernel_size=2, stride=2) # 128 -> 256\n        self.dec_conv3 = nn.Sequential(\n            self._dec_layer(64, 64),\n            nn.ConvTranspose2d(64, 1, kernel_size=(3, 3), padding=(1, 1)),\n            # nn.BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n            # nn.ReLU(inplace=True)\n       )\n\n    def forward(self, x):\n        # encoder\n        e0, idx0 = self.pool0(self.enc_conv0(x))\n        e1, idx1 = self.pool1(self.enc_conv1(e0))\n        e2, idx2 = self.pool2(self.enc_conv2(e1))\n        e3, idx3 = self.pool3(self.enc_conv3(e2))\n\n        # bottleneck\n        # b = self.bottleneck_conv(e3)\n        p, idx_bn = self.pool_bn(self.enc_conv_bn(e3))\n        b = self.dec_conv_bn(self.upsample_bn(p, idx_bn))\n        \n        # decoder\n        d0 = self.dec_conv0(self.upsample0(b, idx3))\n        d1 = self.dec_conv1(self.upsample1(d0, idx2))\n        d2 = self.dec_conv2(self.upsample2(d1, idx1))\n        d3 = self.dec_conv3(self.upsample3(d2, idx0)) # no activation\n        return d3","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.488249Z","iopub.execute_input":"2022-03-01T21:41:42.489077Z","iopub.status.idle":"2022-03-01T21:41:42.512178Z","shell.execute_reply.started":"2022-03-01T21:41:42.489038Z","shell.execute_reply":"2022-03-01T21:41:42.511399Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"segnet = SegNet().to(device)\n# segnet\nsummary(segnet, input_size=(3, 256, 256), batch_size=batch_size)\ndel segnet","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.513561Z","iopub.execute_input":"2022-03-01T21:41:42.514003Z","iopub.status.idle":"2022-03-01T21:41:50.783829Z","shell.execute_reply.started":"2022-03-01T21:41:42.513968Z","shell.execute_reply":"2022-03-01T21:41:50.783063Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# segnet = SegNet().to(device)\n# optim = torch.optim.Adam(segnet.parameters(), lr=1e-4)\n# loss_func = BCELoss_with_logits()\n# #loss_func = FocalTverskyLoss(0.6, 0.4, 2)\n# losses, metric = train_model(segnet, loss_func, optim, dataloader_train, dataloader_test, epochs=20)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:50.785117Z","iopub.execute_input":"2022-03-01T21:41:50.786027Z","iopub.status.idle":"2022-03-01T21:41:50.790281Z","shell.execute_reply.started":"2022-03-01T21:41:50.785984Z","shell.execute_reply":"2022-03-01T21:41:50.789564Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# score_model(segnet, iou_pytorch, dataloader_valid)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:50.791757Z","iopub.execute_input":"2022-03-01T21:41:50.792688Z","iopub.status.idle":"2022-03-01T21:41:50.802297Z","shell.execute_reply.started":"2022-03-01T21:41:50.792632Z","shell.execute_reply":"2022-03-01T21:41:50.801389Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Model validation","metadata":{}},{"cell_type":"code","source":"def validate_model(model_class, model_file, pickle_file, max_epochs=200):\n    loss = {\n        #'bce': BCELoss_with_logits(reduction='mean', truncate=True),\n        'bce': BCELoss_classic(reduction='mean'),\n        #'bce': nn.BCEWithLogitsLoss(reduction='mean'),\n        'dice': DiceLoss(),\n        #'focal': FocalLoss(),\n        'tversky': TverskyLoss(alpha=0.7, beta = 0.3),\n        'focal_tversky': FocalTverskyLoss(alpha=0.6, beta = 0.4, gamma=2),\n        'lovasz': LovaszLoss(),\n    }\n    model_history = {}\n    for loss_name, loss_func in loss.items():\n        model_history[loss_name] = {}\n        model = model_class().to(device)\n        #optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n        optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n        losses, metric = train_model(model, loss_func, optim, dataloader_train, dataloader_test, max_epochs)\n        model_history[loss_name]['losses'] = losses\n        model_history[loss_name]['metric'] = metric\n        torch.save(model, '{}_{}_{}epoch.model'.format(model_file, loss_name, max_epochs))\n    with open(pickle_file, 'wb') as handle:\n        pickle.dump(model_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    return model_history","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:50.807138Z","iopub.execute_input":"2022-03-01T21:41:50.807397Z","iopub.status.idle":"2022-03-01T21:41:50.815822Z","shell.execute_reply.started":"2022-03-01T21:41:50.807302Z","shell.execute_reply":"2022-03-01T21:41:50.814950Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"max_epochs = 200\nsegnet_history = validate_model(SegNet, 'segnet', 'segnet.pickle', max_epochs=max_epochs)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:50.817969Z","iopub.execute_input":"2022-03-01T21:41:50.818589Z","iopub.status.idle":"2022-03-01T22:45:15.871312Z","shell.execute_reply.started":"2022-03-01T21:41:50.818542Z","shell.execute_reply":"2022-03-01T22:45:15.870549Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# with open('../input/temporary/segnet.pickle', 'rb') as handle:\n#     segnet_history = pickle.load(handle)\n# with open('../input/temporary/unet.pickle', 'rb') as handle:\n#     unet_history = pickle.load(handle)\n# with open('../input/temporary/unet2.pickle', 'rb') as handle:\n#     unet2_history = pickle.load(handle)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:45:15.872796Z","iopub.execute_input":"2022-03-01T22:45:15.873047Z","iopub.status.idle":"2022-03-01T22:45:15.876265Z","shell.execute_reply.started":"2022-03-01T22:45:15.873003Z","shell.execute_reply":"2022-03-01T22:45:15.875609Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.set(context='paper')\n\nfor history in [segnet_history]:\n    plt.style.use(\"ggplot\")\n    plt.rcParams['axes.edgecolor'] = \"#777777\"\n    plt.rcParams['axes.facecolor'] = '#FFFFFF'\n    fig, ax = plt.subplots(1, 2, figsize=(12,4))\n    x = list(range(max_epochs))\n    for key, value in history.items():\n        sns.lineplot(x=x, y=value['losses'], ax=ax[0], label=key)\n        sns.lineplot(x=x, y=value['metric'], ax=ax[1], label=key)\n    #ax[0].set_title('loss')\n    #ax[1].set_title('IoU')\n    ax[0].legend()\n    ax[1].legend()\n    ax[0].set_xlabel('epoch')\n    ax[1].set_xlabel('epoch')\n    ax[0].set_ylabel('loss')\n    ax[1].set_ylabel('IoU')\n    plt.show() ","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-01T22:46:48.432405Z","iopub.execute_input":"2022-03-01T22:46:48.432721Z","iopub.status.idle":"2022-03-01T22:46:49.446065Z","shell.execute_reply.started":"2022-03-01T22:46:48.432687Z","shell.execute_reply":"2022-03-01T22:46:49.445212Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"for history in [segnet_history]:\n    plt.rcParams['axes.spines.right'] = False\n    plt.rcParams['axes.spines.top'] = False\n    plt.style.use(\"ggplot\")\n    plt.rcParams['axes.edgecolor'] = \"#777777\"\n    plt.rcParams['axes.facecolor'] = '#FFFFFF'\n    ax = sns.barplot(x=list(history.keys()), y=[max(m['metric']) for m in history.values()])\n    ax.bar_label(ax.containers[0])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:46:55.577824Z","iopub.execute_input":"2022-03-01T22:46:55.578280Z","iopub.status.idle":"2022-03-01T22:46:55.803153Z","shell.execute_reply.started":"2022-03-01T22:46:55.578242Z","shell.execute_reply":"2022-03-01T22:46:55.802473Z"},"trusted":true},"execution_count":34,"outputs":[]}]}
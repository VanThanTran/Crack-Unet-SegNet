{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary gdown\n#!pip install albumentations==0.4.6","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:25:11.593220Z","iopub.execute_input":"2022-03-02T08:25:11.593568Z","iopub.status.idle":"2022-03-02T08:25:39.584922Z","shell.execute_reply.started":"2022-03-02T08:25:11.593423Z","shell.execute_reply":"2022-03-02T08:25:39.583767Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom skimage import io\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as tfs\nimport torchvision.models\nfrom torchsummary import summary\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nfrom IPython.display import clear_output\nfrom tqdm.notebook import tqdm\nimport pickle\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:25:56.605288Z","iopub.execute_input":"2022-03-02T08:25:56.605623Z","iopub.status.idle":"2022-03-02T08:26:01.389627Z","shell.execute_reply.started":"2022-03-02T08:25:56.605582Z","shell.execute_reply":"2022-03-02T08:26:01.388676Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ncontent_dir = '/kaggle/working/PH2Dataset'\nbatch_size = 16","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:26:14.503773Z","iopub.execute_input":"2022-03-02T08:26:14.504588Z","iopub.status.idle":"2022-03-02T08:26:14.510997Z","shell.execute_reply.started":"2022-03-02T08:26:14.504552Z","shell.execute_reply":"2022-03-02T08:26:14.509880Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data","metadata":{}},{"cell_type":"code","source":"if not os.path.exists(content_dir):\n    print('Download dataset...')\n    # !wget --quiet https://www.dropbox.com/s/k88qukc20ljnbuo/PH2Dataset.rar\n    !gdown https://drive.google.com/uc?id=1JHRw-LcPoHoApGNoub2b-DgrSWnRw-ZB -O PH2Dataset.zip\n    # print('Install unrar...', end='')\n    # !apt-get -qq install unrar > /dev/null 2>&1\n    # print('done.')\n    print('Extract archive files...', end='')\n    #!unrar x -idq PH2Dataset.rar\n    !unzip -q PH2Dataset.zip\n    print('done.')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:26:26.938272Z","iopub.execute_input":"2022-03-02T08:26:26.938553Z","iopub.status.idle":"2022-03-02T08:26:31.696601Z","shell.execute_reply.started":"2022-03-02T08:26:26.938522Z","shell.execute_reply":"2022-03-02T08:26:31.695380Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class CustomDataSet(Dataset):\n    def __init__(self, main_dir, transform):\n        self.transform = transform\n        self.samples = []\n        self.labels = []\n        for root, dirs, files in os.walk(main_dir):\n            if root.endswith('_Dermoscopic_Image'):\n                self.samples.append(os.path.join(root, files[0]))\n            if root.endswith('_lesion'):\n                self.labels.append(os.path.join(root, files[0]))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        # image = Image.open(self.samples[idx])\n        # label = Image.open(self.labels[idx])\n        image = cv2.imread(self.samples[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = cv2.imread(self.labels[idx], cv2.IMREAD_GRAYSCALE)\n        transformed = self.transform(image=image, mask=label)\n        tensor_image = transformed['image'].transpose(2, 0, 1)\n        label_image = transformed['mask'][np.newaxis, :]\n        tensor_image = torch.FloatTensor(tensor_image) / 255\n        label_image = torch.FloatTensor(label_image) / 255\n        #print(tensor_image.shape, label_image.shape)\n        #print(tensor_image.min(), tensor_image.max(), label_image.min(), label_image.max())\n        return tensor_image, label_image","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:26:57.976773Z","iopub.execute_input":"2022-03-02T08:26:57.977077Z","iopub.status.idle":"2022-03-02T08:26:57.989196Z","shell.execute_reply.started":"2022-03-02T08:26:57.977047Z","shell.execute_reply":"2022-03-02T08:26:57.987774Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"transform = A.Compose([\n    A.Resize(256, 256),\n#     A.HorizontalFlip(),\n#     A.VerticalFlip(),\n#     A.augmentations.geometric.Affine(\n#         scale={'x':(1.0, 1.2), 'y':(1.0, 1.2)},\n# #        cval=[55, 51, 49],\n# #        mode=cv2.BORDER_CONSTANT\n#     )\n    #A.ToFloat(),\n    #A.pytorch.ToTensorV2(),\n    #A.Normalize(mean=0, std=1),\n    #tfs.RandomAffine(10,translate=(0.1, 0.1), scale=(0.9, 1.1)),\n])\n\n# transform = tfs.Compose([\n#     tfs.Resize((256, 256)),\n#     tfs.ToTensor(),\n# ])\n\ndermoscopic_dataset = CustomDataSet(os.path.join(content_dir, 'PH2 Dataset images'), transform=transform)\n\nidx = np.random.choice(len(dermoscopic_dataset), len(dermoscopic_dataset), False)\n#train_idx, valid_idx, test_idx = np.split(idx, [50, 70])\ntrain_idx, test_idx = np.split(idx, [70])\n\ndataset_train = Subset(dermoscopic_dataset, train_idx)\n#dataset_valid = Subset(dermoscopic_dataset, valid_idx)\ndataset_test  = Subset(dermoscopic_dataset, test_idx)\n\ndataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n#dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=True)\ndataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:27:14.602268Z","iopub.execute_input":"2022-03-02T08:27:14.602700Z","iopub.status.idle":"2022-03-02T08:27:14.642511Z","shell.execute_reply.started":"2022-03-02T08:27:14.602653Z","shell.execute_reply":"2022-03-02T08:27:14.641527Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"def show_dermoscopic_imgs(images, labels, threshold=None):\n    images = images.numpy().transpose(0, 2, 3, 1)\n    labels = labels.numpy().transpose(0, 2, 3, 1)\n    if threshold is not None:\n        labels = np.where(labels > threshold, 1, 0)\n    plt.figure(figsize=(18, 6))\n    for i in range(4):\n        plt.subplot(2, 6, i+1)\n        plt.imshow(images[i])\n        plt.axis(\"off\")\n\n        plt.subplot(2, 6, i+7)\n        plt.imshow(labels[i], cmap='gray')\n        plt.axis(\"off\")\n    # plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:27:31.162755Z","iopub.execute_input":"2022-03-02T08:27:31.163074Z","iopub.status.idle":"2022-03-02T08:27:31.171801Z","shell.execute_reply.started":"2022-03-02T08:27:31.163042Z","shell.execute_reply":"2022-03-02T08:27:31.170701Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"images, labels = next(iter(dataloader_train))\nshow_dermoscopic_imgs(images, labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:27:42.961454Z","iopub.execute_input":"2022-03-02T08:27:42.962225Z","iopub.status.idle":"2022-03-02T08:27:43.938586Z","shell.execute_reply.started":"2022-03-02T08:27:42.962173Z","shell.execute_reply":"2022-03-02T08:27:43.937559Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def show_loss(history):\n    plt.figure(figsize=(12, 8))\n    plt.plot(history)\n    plt.title('Loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:28:22.498969Z","iopub.execute_input":"2022-03-02T08:28:22.499255Z","iopub.status.idle":"2022-03-02T08:28:22.507602Z","shell.execute_reply.started":"2022-03-02T08:28:22.499225Z","shell.execute_reply":"2022-03-02T08:28:22.506317Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Metrics\n\nGood summary: [A survey of loss functions for semantic segmentation](https://arxiv.org/pdf/2006.14822.pdf)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:13:39.409414Z","iopub.execute_input":"2021-11-18T13:13:39.409711Z","iopub.status.idle":"2021-11-18T13:13:39.413556Z","shell.execute_reply.started":"2021-11-18T13:13:39.409677Z","shell.execute_reply":"2021-11-18T13:13:39.412683Z"}}},{"cell_type":"markdown","source":"## IoU scoring metric\n\n$$I o U=\\frac{\\text {target } \\cap \\text { prediction }}{\\text {target } \\cup{prediction }}$$","metadata":{}},{"cell_type":"code","source":"SMOOTH = 1e-6\n# https://www.kaggle.com/iezepov/fast-iou-scoring-metric-in-pytorch-and-numpy\ndef iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n    outputs = outputs.byte()\n    labels = labels.byte()\n    #print(outputs)\n    # You can comment out this line if you are passing tensors of equal shape\n    # But if you are passing output from UNet or something it will most probably\n    # be with the BATCH x 1 x H x W shape\n    #outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n    # thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n    # return thresholded  # Or thresholded.mean() if you are interested in average across the batch\n    return iou","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:28:27.650900Z","iopub.execute_input":"2022-03-02T08:28:27.655783Z","iopub.status.idle":"2022-03-02T08:28:27.669693Z","shell.execute_reply.started":"2022-03-02T08:28:27.655735Z","shell.execute_reply":"2022-03-02T08:28:27.668644Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## BCE Loss\n\n$$\\mathcal L_{BCE}(y, \\hat y) = -\\sum_i \\left[y_i\\log\\sigma(\\hat y_i) + (1-y_i)\\log(1-\\sigma(\\hat y_i))\\right]$$","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:27:55.204072Z","iopub.execute_input":"2021-11-18T13:27:55.204598Z","iopub.status.idle":"2021-11-18T13:27:55.207505Z","shell.execute_reply.started":"2021-11-18T13:27:55.204565Z","shell.execute_reply":"2021-11-18T13:27:55.206925Z"}}},{"cell_type":"code","source":"class BCELoss_classic(nn.Module):\n    def __init__(self, reduction='mean'):\n        super().__init__()\n        if reduction not in ('mean', 'sum'):\n            raise ValueError('\"{}\" is not a valid mode for reduction. Only \"mean\"'\n                             'and \"sum\" are allowed.'.format(rediction))\n        self.reduction = reduction\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        # outputs = torch.clamp(outputs, 0, 1)\n        outputs = torch.sigmoid(outputs)\n        bce = labels * torch.log(outputs + SMOOTH) + (1 - labels) * torch.log(1 - outputs + SMOOTH)\n        if self.reduction == 'mean':\n            return -torch.mean(bce)\n        elif self.reduction == 'sum':\n            return -torch.sum(bce)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:28:31.122470Z","iopub.execute_input":"2022-03-02T08:28:31.123271Z","iopub.status.idle":"2022-03-02T08:28:31.131533Z","shell.execute_reply.started":"2022-03-02T08:28:31.123235Z","shell.execute_reply":"2022-03-02T08:28:31.130211Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"$$\\mathcal L_{BCE}(y, \\hat y) = \\hat y - y\\hat y + \\log\\left(1+\\exp(-\\hat y)\\right)$$","metadata":{}},{"cell_type":"code","source":"class BCELoss_with_logits(nn.Module):\n    def __init__(self, reduction='mean', truncate=False):\n        super().__init__()\n        if reduction not in ('mean', 'sum'):\n            raise ValueError('\"{}\" is not a valid mode for reduction. Only \"mean\"'\n                             'and \"sum\" are allowed.'.format(rediction))\n        self.reduction = reduction\n        self.truncate = truncate\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        # classical without sigmoid\n        # but you need to cut off large negative logits, otherwise it will be -inf -> nan \n        if self.truncate:\n            outputs = torch.sigmoid(outputs)\n        outputs = outputs.float()\n        labels = labels.float()\n        # print(torch.min(outputs))\n        bce = outputs - labels * outputs + torch.log(1 + torch.exp(-outputs))\n        if self.reduction == 'mean':\n            return torch.mean(bce)\n        elif self.reduction == 'sum':\n            return torch.sum(bce)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:28:34.987696Z","iopub.execute_input":"2022-03-02T08:28:34.987981Z","iopub.status.idle":"2022-03-02T08:28:34.997887Z","shell.execute_reply.started":"2022-03-02T08:28:34.987952Z","shell.execute_reply":"2022-03-02T08:28:34.996681Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# torch.mean(torch.log(1 + torch.exp(torch.tensor(255))))","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:41:42.274325Z","iopub.execute_input":"2022-03-01T21:41:42.274654Z","iopub.status.idle":"2022-03-01T21:41:42.285349Z","shell.execute_reply.started":"2022-03-01T21:41:42.274585Z","shell.execute_reply":"2022-03-01T21:41:42.284662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = torch.Tensor([\n    [-0.4717,  0.8484,  0.7424],\n    [ 0.0880,  0.1379,  0.8387],\n    [ 0.3874, -1.8205,  1.5422]\n])\ntarget = torch.Tensor([\n    [0, 1, 0],\n    [1, 0, 1],\n    [0, 1, 0]\n])\nsigm = nn.Sigmoid()\n\nprint(\n    output,\n    target,\n    nn.BCELoss(reduction='mean')(sigm(output), target),\n    BCELoss_classic(reduction='mean')(output, target),\n    BCELoss_with_logits(reduction='mean')(output, target),\n    sep='\\n'\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:28:39.010185Z","iopub.execute_input":"2022-03-02T08:28:39.010744Z","iopub.status.idle":"2022-03-02T08:28:39.108696Z","shell.execute_reply.started":"2022-03-02T08:28:39.010710Z","shell.execute_reply":"2022-03-02T08:28:39.107381Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Dice Loss\n\n$$D(X,Y)=\\frac{2|X\\cap Y|}{|X|+|Y|}$$\n\n$$\\mathcal L_D(X,Y) = 1-\\frac{1}{256 \\times 256} \\times \\sum_i\\frac{2X_iY_i}{X_i+Y_i}.$$","metadata":{}},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, reduction='mean'):\n        super().__init__()\n\n#     # not work, why?\n#     def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n#         coef = 1 / (outputs.shape[0] * outputs.shape[2] * outputs.shape[3])\n#         outputs = torch.sigmoid(outputs)\n#         num = 2 * outputs * labels\n#         den = outputs + labels\n#         res = 1 - coef * ((num + 1)/(den + 1)).sum()\n#         return res\n\n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        outputs = torch.sigmoid(outputs)\n        num = 2 * (outputs * labels).sum()\n        den = (outputs + labels).sum()\n        res = 1 - (num + SMOOTH) / (den + SMOOTH)\n        return res","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:28:42.442837Z","iopub.execute_input":"2022-03-02T08:28:42.443244Z","iopub.status.idle":"2022-03-02T08:28:42.452907Z","shell.execute_reply.started":"2022-03-02T08:28:42.443198Z","shell.execute_reply":"2022-03-02T08:28:42.451907Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Focal Loss\n\n$$FL(p_t) = -\\alpha_t(1-p_t)^\\gamma log(p_t)$$\n$$CE(p,y) = CE(p_t) = -log(p_t)$$\n$$p_t = e^{-CE(p_t)}$$\n$$FL(p_t) = \\alpha_t(1-e^{-CE(p_t)})^\\gamma CE(p_t)$$","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:15:44.629785Z","iopub.execute_input":"2021-11-21T09:15:44.630127Z","iopub.status.idle":"2021-11-21T09:15:44.635131Z","shell.execute_reply.started":"2021-11-21T09:15:44.630085Z","shell.execute_reply":"2021-11-21T09:15:44.634061Z"}}},{"cell_type":"code","source":"# https://arxiv.org/pdf/1708.02002.pdf\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha: int = 1, gamma: int = 2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        bce_logit = nn.BCEWithLogitsLoss()\n        ce = bce_logit(outputs, labels)\n        pt = torch.exp(-ce)\n        fl = self.alpha * torch.pow((1 - pt), self.gamma) * ce\n        return fl","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:28:45.481579Z","iopub.execute_input":"2022-03-02T08:28:45.481903Z","iopub.status.idle":"2022-03-02T08:28:45.490254Z","shell.execute_reply.started":"2022-03-02T08:28:45.481871Z","shell.execute_reply":"2022-03-02T08:28:45.489267Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Tversky Loss\n\n[Tversky loss function for image segmentation using 3D fully convolutional deep networks](https://arxiv.org/abs/1706.05721)\n\n$$TI(p,\\hat p) = \\frac{p\\hat p}{p\\hat p + \\beta(1 − p)\\hat p + (1 − \\beta)p(1 − \\hat p)}$$\n$$TL(p,\\hat p) = 1 - \\frac{1 + p\\hat p}{1+ p\\hat p + \\beta(1 − p)\\hat p + (1 − \\beta)p(1 − \\hat p)}$$","metadata":{}},{"cell_type":"code","source":"class TverskyLoss(nn.Module):\n    def __init__(self, alpha:int = 0.5, beta:int = 0.5):\n        super().__init__()\n        self.alpha = alpha\n        self.beta = beta\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        outputs = torch.sigmoid(outputs)\n        pp = (labels * outputs).sum()\n        den1 = self.alpha * ((1 - labels) * outputs).sum()\n        den2 = self.beta * (labels * (1 - outputs)).sum()\n        tl = 1 - (1 + pp) / (1 + pp + den1 + den2)\n        return tl      ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:28:48.914024Z","iopub.execute_input":"2022-03-02T08:28:48.914895Z","iopub.status.idle":"2022-03-02T08:28:48.924766Z","shell.execute_reply.started":"2022-03-02T08:28:48.914859Z","shell.execute_reply":"2022-03-02T08:28:48.923371Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Focal Tversky Loss","metadata":{}},{"cell_type":"code","source":"class FocalTverskyLoss(nn.Module):\n    def __init__(self, alpha:int = 0.5, beta:int = 0.5, gamma:int = 2):\n        super().__init__()\n        self.gamma = gamma\n        self.tl = TverskyLoss(alpha, beta)\n        \n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        tl = self.tl(outputs, labels)\n        return torch.pow(tl, self.gamma)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:28:52.097303Z","iopub.execute_input":"2022-03-02T08:28:52.097760Z","iopub.status.idle":"2022-03-02T08:28:52.104452Z","shell.execute_reply.started":"2022-03-02T08:28:52.097725Z","shell.execute_reply":"2022-03-02T08:28:52.103551Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Lovasz Loss","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/67791\nclass LovaszLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, outputs: torch.Tensor, labels: torch.Tensor):\n        #outputs = torch.sigmoid(outputs)\n        outputs = outputs.flatten()\n        labels = labels.flatten()\n        signs = 2 * labels.float() - 1\n        errors = (1 - outputs * signs)\n        errors_sorted, indices = torch.sort(errors, dim=0, descending=True)\n        gt_sorted = labels[indices.data]\n\n        # gradient\n        gts = gt_sorted.sum()\n        intersection = gts - gt_sorted.float().cumsum(0)\n        union = gts + (1 - gt_sorted).float().cumsum(0)\n        grad = 1. - intersection / union\n\n        p = len(gt_sorted)\n        grad[1:p] = grad[1:p] - grad[0:-1]\n       \n        loss = torch.dot(torch.relu(errors_sorted), grad)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:28:55.266939Z","iopub.execute_input":"2022-03-02T08:28:55.267251Z","iopub.status.idle":"2022-03-02T08:28:55.278565Z","shell.execute_reply.started":"2022-03-02T08:28:55.267219Z","shell.execute_reply":"2022-03-02T08:28:55.277581Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Train/valid functions","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:33:36.596973Z","iopub.execute_input":"2021-11-21T13:33:36.598099Z","iopub.status.idle":"2021-11-21T13:33:36.625852Z","shell.execute_reply.started":"2021-11-21T13:33:36.597927Z","shell.execute_reply":"2021-11-21T13:33:36.624662Z"}}},{"cell_type":"code","source":"def score_model(model, metric, data, threshold=0):\n    model.to(device).eval() # testing mode\n    scores = 0\n    threshold = torch.tensor(threshold).to(device)\n    for X_batch, Y_label in data:\n        X_batch = X_batch.to(device)\n        with torch.no_grad():\n            Y_pred = model(X_batch)\n        scores += metric((Y_pred > threshold), Y_label.to(device)).mean().item()\n    return scores/len(data)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:29:03.947321Z","iopub.execute_input":"2022-03-02T08:29:03.947933Z","iopub.status.idle":"2022-03-02T08:29:03.956240Z","shell.execute_reply.started":"2022-03-02T08:29:03.947899Z","shell.execute_reply":"2022-03-02T08:29:03.953843Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def train_model(\n    model: torch.nn.Module,\n    criterion: torch.nn.Module,\n    optimizer: torch.nn.Module,\n    dataloader_train: torch.utils.data.DataLoader,\n    dataloader_test: torch.utils.data.DataLoader,\n    epochs: int = 100\n) -> (torch.nn.Module, dict):\n    r\"\"\"Training the model. Returns list of train losses.\n    Args:\n        model (torch.nn.Module): Neural network\n        criterion (torch.nn.Module): Cost function\n        optimizer (torch.nn.Module): Optimization algorithm\n        dataloader_train: (torch.utils.data.DataLoader): Train data\n        dataloader_valid: (torch.utils.data.DataLoader): Valid data\n        epochs (int): Number of training iterations. Default: 20\n    \"\"\"\n#def train(model, optimizer, loss_fn, epochs, data_tr, data_val):\n    X_val, Y_val = next(iter(dataloader_test))\n    losses = []\n    metric = []\n  \n    for epoch in range(epochs):\n        avg_loss = 0\n        model.train()  # train mode\n        for X_batch, Y_batch in tqdm(dataloader_train, desc='Progress'):\n            # data to device\n            X_batch = X_batch.to(device)\n            Y_batch = Y_batch.to(device)\n            # set parameter gradients to zero\n            optimizer.zero_grad()\n            # forward\n            # print(Y_batch.shape)\n            Y_pred = model(X_batch)\n            loss = criterion(Y_pred, Y_batch)\n            print(loss)\n            loss.backward() # backward-pass\n            optimizer.step()  # update weights\n            # calculate loss to show the user\n            avg_loss += loss / len(dataloader_train)\n\n        losses.append(avg_loss.item())\n        metric.append(score_model(model, iou_pytorch, dataloader_test))\n        \n        # show intermediate results\n        model.eval()  # testing mode\n        # detach and put into cpu\n        Y_hat = model(X_val.to(device)).detach().cpu()\n        # Visualize tools\n        clear_output(wait=True)\n        show_dermoscopic_imgs(X_val, Y_hat, threshold=0.1)\n        plt.title('%d / %d - loss: %f' % (epoch+1, epochs, avg_loss))\n        plt.show()\n    return losses, metric","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:29:14.266345Z","iopub.execute_input":"2022-03-02T08:29:14.266703Z","iopub.status.idle":"2022-03-02T08:29:14.279277Z","shell.execute_reply.started":"2022-03-02T08:29:14.266669Z","shell.execute_reply":"2022-03-02T08:29:14.278160Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def predict(model, data):\n    model.to(device).eval()  # testing mode\n    with torch.no_grad():\n        Y_pred = [model(X_batch.to(device)) for X_batch, _ in data]\n    return Y_pred","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:29:18.921916Z","iopub.execute_input":"2022-03-02T08:29:18.922231Z","iopub.status.idle":"2022-03-02T08:29:18.929410Z","shell.execute_reply.started":"2022-03-02T08:29:18.922199Z","shell.execute_reply":"2022-03-02T08:29:18.928308Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# UNet","metadata":{}},{"cell_type":"code","source":"class UNet(nn.Module):\n    \n    def _conv_conv(self,in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n    \n    def _enc_layer(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            self._conv_conv(in_channels, out_channels)\n        )\n    \n    def __init__(self):\n        super().__init__()\n        \n        # encoder (downsampling)\n        # Each enc_conv/dec_conv block should look like this:\n        # nn.Sequential(\n        #     nn.Conv2d(...),\n        #     ... (2 or 3 conv layers with relu and batchnorm),\n        # )\n        \n        self.enc_conv0 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.enc_lr0 = self._enc_layer(64, 128)\n        self.enc_lr1 = self._enc_layer(128, 256)\n        self.enc_lr2 = self._enc_layer(256, 512)\n        self.enc_lr3 = self._enc_layer(512, 512)\n        # decoder (upsampling)\n        self.upsample0 = nn.Upsample(scale_factor=2)#ConvTranspose2d(1024, 512, kernel_size=2, stride=2) # 16 -> 32\n        self.dec_conv0 = self._conv_conv(2 * 512, 256)\n        self.upsample1 = nn.Upsample(scale_factor=2)#nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2) # 32 -> 64\n        self.dec_conv1 = self._conv_conv(2 * 256, 128)\n        self.upsample2 = nn.Upsample(scale_factor=2)#nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) # 64 -> 128\n        self.dec_conv2 = self._conv_conv(2 * 128, 64)\n        self.upsample3 = nn.Upsample(scale_factor=2)#nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)  # 128 -> 256\n        self.dec_conv3 = nn.Sequential(\n            self._conv_conv(2 * 64, 64),\n            nn.Conv2d(64, 1, kernel_size=1),\n        )\n\n    def forward(self, x):\n        # encoder\n        e0 = self.enc_conv0(x)\n        e1 = self.enc_lr0(e0)\n        e2 = self.enc_lr1(e1)\n        e3 = self.enc_lr2(e2)\n        # bottleneck\n        b = self.upsample0(self.enc_lr3(e3))\n\n        # decoder\n        d0 = self.upsample1(self.dec_conv0(torch.cat((b, e3), dim=1)))\n        d1 = self.upsample2(self.dec_conv1(torch.cat((d0, e2), dim=1)))\n        d2 = self.upsample3(self.dec_conv2(torch.cat((d1, e1), dim=1)))\n        d3 = self.dec_conv3(torch.cat((d2, e0), dim=1))  # no activation\n        return d3","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:30:41.300106Z","iopub.execute_input":"2022-03-02T08:30:41.300698Z","iopub.status.idle":"2022-03-02T08:30:41.321997Z","shell.execute_reply.started":"2022-03-02T08:30:41.300641Z","shell.execute_reply":"2022-03-02T08:30:41.320685Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"unet = UNet().to(device)\n# unet\nsummary(unet, input_size=(3, 256, 256), batch_size=batch_size)\ndel unet","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:30:55.202795Z","iopub.execute_input":"2022-03-02T08:30:55.203331Z","iopub.status.idle":"2022-03-02T08:31:03.907638Z","shell.execute_reply.started":"2022-03-02T08:30:55.203295Z","shell.execute_reply":"2022-03-02T08:31:03.906544Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"unet = UNet().to(device)\noptim = torch.optim.Adam(unet.parameters(), lr=1e-4)\n#optim = torch.optim.SGD(unet.parameters(), lr=0.01, momentum=0.9)\nloss_func = LovaszLoss()\nlosses, metric = train_model(unet, loss_func, optim, dataloader_train, dataloader_test, epochs=100)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:31:43.786109Z","iopub.execute_input":"2022-03-02T08:31:43.786408Z","iopub.status.idle":"2022-03-02T08:37:32.377409Z","shell.execute_reply.started":"2022-03-02T08:31:43.786378Z","shell.execute_reply":"2022-03-02T08:37:32.376556Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# UNet2","metadata":{}},{"cell_type":"code","source":"class UNet2(nn.Module):\n    \n    def _conv_conv(self,in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n    \n    def _enc_layer(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=1),\n            self._conv_conv(in_channels, out_channels)\n        )\n    \n    def __init__(self):\n        super().__init__()\n        \n        # encoder (downsampling)\n        # Each enc_conv/dec_conv block should look like this:\n        # nn.Sequential(\n        #     nn.Conv2d(...),\n        #     ... (2 or 3 conv layers with relu and batchnorm),\n        # )\n        \n        self.enc_conv0 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.enc_lr0 = self._enc_layer(64, 128)\n        self.enc_lr1 = self._enc_layer(128, 256)\n        self.enc_lr2 = self._enc_layer(256, 512)\n        self.enc_lr3 = self._enc_layer(512, 1024)\n        # decoder (upsampling)\n        self.upsample0 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2) # 16 -> 32\n        self.dec_conv0 = self._conv_conv(2 * 512, 512)\n        self.upsample1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2) # 32 -> 64\n        self.dec_conv1 = self._conv_conv(2 * 256, 256)\n        self.upsample2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) # 64 -> 128\n        self.dec_conv2 = self._conv_conv(2 * 128, 128)\n        self.upsample3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)  # 128 -> 256\n        self.dec_conv3 = nn.Sequential(\n            self._conv_conv(2 * 64, 64),\n            nn.Conv2d(64, 1, kernel_size=1),\n        )\n\n    def forward(self, x):\n        # encoder\n        e0 = self.enc_conv0(x)\n        e1 = self.enc_lr0(e0)\n        e2 = self.enc_lr1(e1)\n        e3 = self.enc_lr2(e2)\n        # bottleneck\n        b = self.upsample0(self.enc_lr3(e3))\n\n        # decoder\n        d0 = self.upsample1(self.dec_conv0(torch.cat((b, e3), dim=1)))\n        d1 = self.upsample2(self.dec_conv1(torch.cat((d0, e2), dim=1)))\n        d2 = self.upsample3(self.dec_conv2(torch.cat((d1, e1), dim=1)))\n        d3 = self.dec_conv3(torch.cat((d2, e0), dim=1))  # no activation\n        return d3","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:38:58.314166Z","iopub.execute_input":"2022-03-02T08:38:58.314584Z","iopub.status.idle":"2022-03-02T08:38:58.350517Z","shell.execute_reply.started":"2022-03-02T08:38:58.314536Z","shell.execute_reply":"2022-03-02T08:38:58.349178Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"unet2 = UNet2().to(device)\n# unet2\nsummary(unet2, input_size=(3, 256, 256), batch_size=batch_size)\ndel unet2","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:39:04.062748Z","iopub.execute_input":"2022-03-02T08:39:04.063128Z","iopub.status.idle":"2022-03-02T08:39:04.453734Z","shell.execute_reply.started":"2022-03-02T08:39:04.063064Z","shell.execute_reply":"2022-03-02T08:39:04.452684Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"unet2 = UNet2().to(device)\noptim = torch.optim.Adam(unet2.parameters(), lr=1e-4)\n#optim = torch.optim.SGD(unet2.parameters(), lr=0.01, momentum=0.9)\nloss_func = LovaszLoss()\nlosses, metric = train_model(unet2, loss_func, optim, dataloader_train, dataloader_test, epochs=100)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:39:09.246455Z","iopub.execute_input":"2022-03-02T08:39:09.247086Z","iopub.status.idle":"2022-03-02T08:47:24.517676Z","shell.execute_reply.started":"2022-03-02T08:39:09.247006Z","shell.execute_reply":"2022-03-02T08:47:24.516673Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Model validation","metadata":{}},{"cell_type":"code","source":"def validate_model(model_class, model_file, pickle_file, max_epochs=100):\n    loss = {\n        #'bce': BCELoss_with_logits(reduction='mean', truncate=True),\n        'bce': BCELoss_classic(reduction='mean'),\n        #'bce': nn.BCEWithLogitsLoss(reduction='mean'),\n        'dice': DiceLoss(),\n        'focal': FocalLoss(),\n        'tversky': TverskyLoss(alpha=0.7, beta = 0.3),\n        'focal_tversky': FocalTverskyLoss(alpha=0.6, beta = 0.4, gamma=2),\n        'lovasz': LovaszLoss(),\n    }\n    model_history = {}\n    for loss_name, loss_func in loss.items():\n        model_history[loss_name] = {}\n        model = model_class().to(device)\n        #optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n        optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n        losses, metric = train_model(model, loss_func, optim, dataloader_train, dataloader_test, max_epochs)\n        model_history[loss_name]['losses'] = losses\n        model_history[loss_name]['metric'] = metric\n        torch.save(model, '{}_{}_{}epoch.model'.format(model_file, loss_name, max_epochs))\n    with open(pickle_file, 'wb') as handle:\n        pickle.dump(model_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    return model_history","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:47:48.044160Z","iopub.execute_input":"2022-03-02T08:47:48.044722Z","iopub.status.idle":"2022-03-02T08:47:48.056668Z","shell.execute_reply.started":"2022-03-02T08:47:48.044644Z","shell.execute_reply":"2022-03-02T08:47:48.054903Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"UNet","metadata":{}},{"cell_type":"code","source":"max_epochs = 200\nunet_history = validate_model(UNet, 'unet', 'unet.pickle', max_epochs=max_epochs)\n#unet2_history = validate_model(UNet2, 'unet2', 'unet2.pickle', max_epochs=max_epochs)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:48:01.843735Z","iopub.execute_input":"2022-03-02T08:48:01.844041Z","iopub.status.idle":"2022-03-02T09:55:47.625603Z","shell.execute_reply.started":"2022-03-02T08:48:01.844011Z","shell.execute_reply":"2022-03-02T09:55:47.624333Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.set(context='paper')\n\nfor history in [unet_history]:\n    plt.style.use(\"ggplot\")\n    plt.rcParams['axes.edgecolor'] = \"#777777\"\n    plt.rcParams['axes.facecolor'] = '#FFFFFF'\n    fig, ax = plt.subplots(1, 2, figsize=(12,4))\n    x = list(range(max_epochs))\n    for key, value in history.items():\n        sns.lineplot(x=x, y=value['losses'], ax=ax[0], label=key)\n        sns.lineplot(x=x, y=value['metric'], ax=ax[1], label=key)\n    #ax[0].set_title('loss')\n    #ax[1].set_title('IoU')\n    ax[0].legend()\n    ax[1].legend()\n    ax[0].set_xlabel('epoch')\n    ax[1].set_xlabel('epoch')\n    ax[0].set_ylabel('loss')\n    ax[1].set_ylabel('IoU')\n    plt.show() ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T09:58:59.521917Z","iopub.execute_input":"2022-03-02T09:58:59.522214Z","iopub.status.idle":"2022-03-02T09:59:01.097493Z","shell.execute_reply.started":"2022-03-02T09:58:59.522182Z","shell.execute_reply":"2022-03-02T09:59:01.096534Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":" for history in [unet_history,]:\n    plt.rcParams['axes.spines.right'] = False\n    plt.rcParams['axes.spines.top'] = False\n    plt.style.use(\"ggplot\")\n    plt.rcParams['axes.edgecolor'] = \"#777777\"\n    plt.rcParams['axes.facecolor'] = '#FFFFFF'\n    ax = sns.barplot(x=list(history.keys()), y=[max(m['metric']) for m in history.values()])\n    ax.bar_label(ax.containers[0])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T09:59:07.297079Z","iopub.execute_input":"2022-03-02T09:59:07.297416Z","iopub.status.idle":"2022-03-02T09:59:07.611292Z","shell.execute_reply.started":"2022-03-02T09:59:07.297383Z","shell.execute_reply":"2022-03-02T09:59:07.610236Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"UNet2","metadata":{}},{"cell_type":"code","source":"max_epochs = 200\n#unet_history = validate_model(UNet, 'unet', 'unet.pickle', max_epochs=max_epochs)\nunet2_history = validate_model(UNet2, 'unet2', 'unet2.pickle', max_epochs=max_epochs)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T10:00:21.945471Z","iopub.execute_input":"2022-03-02T10:00:21.946145Z","iopub.status.idle":"2022-03-02T11:42:41.780351Z","shell.execute_reply.started":"2022-03-02T10:00:21.946108Z","shell.execute_reply":"2022-03-02T11:42:41.779250Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.set(context='paper')\n\nfor history in [unet2_history]:\n    plt.style.use(\"ggplot\")\n    plt.rcParams['axes.edgecolor'] = \"#777777\"\n    plt.rcParams['axes.facecolor'] = '#FFFFFF'\n    fig, ax = plt.subplots(1, 2, figsize=(12,4))\n    x = list(range(max_epochs))\n    for key, value in history.items():\n        sns.lineplot(x=x, y=value['losses'], ax=ax[0], label=key)\n        sns.lineplot(x=x, y=value['metric'], ax=ax[1], label=key)\n    #ax[0].set_title('loss')\n    #ax[1].set_title('IoU')\n    ax[0].legend()\n    ax[1].legend()\n    ax[0].set_xlabel('epoch')\n    ax[1].set_xlabel('epoch')\n    ax[0].set_ylabel('loss')\n    ax[1].set_ylabel('IoU')\n    plt.show() ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T11:42:50.930014Z","iopub.execute_input":"2022-03-02T11:42:50.930285Z","iopub.status.idle":"2022-03-02T11:42:52.184881Z","shell.execute_reply.started":"2022-03-02T11:42:50.930256Z","shell.execute_reply":"2022-03-02T11:42:52.183993Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"for history in [unet2_history,]:\n    plt.rcParams['axes.spines.right'] = False\n    plt.rcParams['axes.spines.top'] = False\n    plt.style.use(\"ggplot\")\n    plt.rcParams['axes.edgecolor'] = \"#777777\"\n    plt.rcParams['axes.facecolor'] = '#FFFFFF'\n    ax = sns.barplot(x=list(history.keys()), y=[max(m['metric']) for m in history.values()])\n    ax.bar_label(ax.containers[0])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T11:42:58.488072Z","iopub.execute_input":"2022-03-02T11:42:58.488388Z","iopub.status.idle":"2022-03-02T11:42:58.784570Z","shell.execute_reply.started":"2022-03-02T11:42:58.488356Z","shell.execute_reply":"2022-03-02T11:42:58.783597Z"},"trusted":true},"execution_count":36,"outputs":[]}]}